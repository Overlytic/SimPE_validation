{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f1e7f4-8657-4272-857e-ee18363dd464",
   "metadata": {},
   "source": [
    "# Salabim: PE Simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37cd2193-a39c-4f34-bb43-ee209af56983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Priority Queue\n",
    "# 221: Changed to Salabim version 21.1.5 to 22.0.2. Now has support for datetime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6904f29f-f46b-4d81-9083-c36ce298d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "import simpy\n",
    "import salabim as sim\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5c095b-4eaf-49af-927b-9b4008f7b1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Add backfilling for different priority jobs\n",
    "## Ability to perform multiple backfilling on same time period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e6b5b-dc2b-4dcf-a918-4a5bf173c29a",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cbbe34e-29cd-4d7c-b469-368e08a54d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# General Settings\n",
    "# -------------------------------------\n",
    "\n",
    "CLUSTER_PE_AVAIL = 8  # Think about how this relates to job trace scaling ... Normally=5 vs but now changed to 1.\n",
    "\n",
    "VERBOSE = False\n",
    "TRACE = False\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# Old Job Generator Settings (not used)\n",
    "# -------------------------------------\n",
    "\n",
    "RAND_SEED = 42\n",
    "\n",
    "# JOB_INTER = 1\n",
    "\n",
    "# JOB_INTER_MEAN = 10 # minutes\n",
    "\n",
    "# JOB_PE_REQ = 1\n",
    "# JOB_RUN_TIME = 30\n",
    "# SIM_TIME = 50\n",
    "\n",
    "# priorities = '12'\n",
    "# priorities_all = list(priorities) + ['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248bc2ea-cf34-4681-990a-e0289e0415a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# path_base = r\"/mnt/wsl/instances/Ubuntu-18.04/home/slurm/slurm_sim_ws/slurm_batches/simple_traces/srun\"\n",
    "path_base = r\"/mnt/wsl/instances/Ubuntu-18.04//home/slurm/slurm_sim_ws/slurm_sim_tools/slurm_batches/simple_traces/srun\"\n",
    "\n",
    "run_type = 'srun8'\n",
    "run_model = \"salabim\"\n",
    "\n",
    "run_num = \"run2\"\n",
    "run_name = f\"srun8_prio2run-2jt1-3-prioprio-run30s-bRhotune2-rho60-200j-{run_num}\"    # get from Thesis: models and variables\n",
    "run_short = f\"fiforun_2jt-run30-rho60-{run_num}\"   # name of graph\n",
    "\n",
    "# SimPE Settings: \n",
    "\n",
    "QUEUE_TYPE = \"Priority\" # \"FIFO\"|\"Priority\"|\"HighPE_Prio\"|\"Backfill\"|\"Backfill_Prio\"\n",
    "\n",
    "UPDATE_DATA_TO_FIFO = True # user, jobprio, sim_account\n",
    "\n",
    "# Save\n",
    "save_data = True\n",
    "thesis_path = Path(\".\").absolute().parent\n",
    "save_image_path = thesis_path / \"images\"\n",
    "save_data_path = save_image_path / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ce9ea57-a14e-4046-981d-890286e0f07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/overlytic/thesis/notebooks/visualise_runs/images/data')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a99c2d-01ad-4bf3-b994-e98364c834ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ls $path_base | grep $run_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9011acab-5fc7-4dae-9289-12788d8a026a",
   "metadata": {},
   "source": [
    "## Job Roster from Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77898938-43b0-49e0-a2ab-ecd215eb9f3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Add backfilling for different priority jobs\n",
    "## Ability to perform multiple backfilling on same time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55272d17-9659-4776-8073-be2837c6d939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/wsl/instances/Ubuntu-18.04//home/slurm/slurm_sim_ws/slurm_sim_tools/slurm_batches/simple_traces/srun/srun8_prio2run-2jt1-3-prioprio-run30s-bRhotune2-rho60-200j-run2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim_job_id</th>\n",
       "      <th>sim_submit</th>\n",
       "      <th>sim_wclimit</th>\n",
       "      <th>sim_duration</th>\n",
       "      <th>sim_tasks</th>\n",
       "      <th>sim_tasks_per_node</th>\n",
       "      <th>sim_username</th>\n",
       "      <th>sim_submit_ts</th>\n",
       "      <th>sim_qosname</th>\n",
       "      <th>sim_partition</th>\n",
       "      <th>sim_account</th>\n",
       "      <th>sim_req_mem</th>\n",
       "      <th>sim_req_mem_per_cpu</th>\n",
       "      <th>sim_features</th>\n",
       "      <th>sim_gres</th>\n",
       "      <th>sim_shared</th>\n",
       "      <th>sim_cpus_per_task</th>\n",
       "      <th>sim_dependency</th>\n",
       "      <th>sim_cancelled_ts</th>\n",
       "      <th>freq</th>\n",
       "      <th>pe</th>\n",
       "      <th>job_prio</th>\n",
       "      <th>nodes_req</th>\n",
       "      <th>batch_name</th>\n",
       "      <th>batch_value</th>\n",
       "      <th>JOBINFO_INTERARRIVAL_TIME</th>\n",
       "      <th>JOBINFO_INTERARRIVAL_MEAN</th>\n",
       "      <th>JOBINFO_INTERARRIVAL_TYPE</th>\n",
       "      <th>CLUSTINFO_NODE_COUNT</th>\n",
       "      <th>CLUSTINFO_NODE_CPUS</th>\n",
       "      <th>CLUSTINFO_NODE_MEM_MB</th>\n",
       "      <th>CLUSTINFO_NODE_DEF_MEM_PER_CPU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>2020-01-01T12:00:02Z</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>user-001</td>\n",
       "      <td>1577880002</td>\n",
       "      <td>normal</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>7424</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.885638</td>\n",
       "      <td>13</td>\n",
       "      <td>exp+1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>59392</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2020-01-01T12:00:10Z</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>user-011</td>\n",
       "      <td>1577880010</td>\n",
       "      <td>normal</td>\n",
       "      <td>training</td>\n",
       "      <td>crucial</td>\n",
       "      <td>22272</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>7.656786</td>\n",
       "      <td>13</td>\n",
       "      <td>exp+1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>59392</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>2020-01-01T12:00:12Z</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>user-001</td>\n",
       "      <td>1577880012</td>\n",
       "      <td>normal</td>\n",
       "      <td>training</td>\n",
       "      <td>training</td>\n",
       "      <td>22272</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2.787696</td>\n",
       "      <td>13</td>\n",
       "      <td>exp+1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>59392</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>2020-01-01T12:00:18Z</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>user-011</td>\n",
       "      <td>1577880018</td>\n",
       "      <td>normal</td>\n",
       "      <td>training</td>\n",
       "      <td>crucial</td>\n",
       "      <td>22272</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>5.751022</td>\n",
       "      <td>13</td>\n",
       "      <td>exp+1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>59392</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>2020-01-01T12:00:32Z</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>user-011</td>\n",
       "      <td>1577880032</td>\n",
       "      <td>normal</td>\n",
       "      <td>training</td>\n",
       "      <td>crucial</td>\n",
       "      <td>22272</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13.848242</td>\n",
       "      <td>13</td>\n",
       "      <td>exp+1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>59392</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sim_job_id            sim_submit  sim_wclimit  sim_duration  sim_tasks  \\\n",
       "0        1000  2020-01-01T12:00:02Z            2           102          1   \n",
       "1        1001  2020-01-01T12:00:10Z            1             2          3   \n",
       "2        1002  2020-01-01T12:00:12Z            1            12          3   \n",
       "3        1003  2020-01-01T12:00:18Z            1             6          3   \n",
       "4        1004  2020-01-01T12:00:32Z            1            14          3   \n",
       "\n",
       "   sim_tasks_per_node sim_username  sim_submit_ts sim_qosname sim_partition  \\\n",
       "0                   1     user-001     1577880002      normal      training   \n",
       "1                   3     user-011     1577880010      normal      training   \n",
       "2                   3     user-001     1577880012      normal      training   \n",
       "3                   3     user-011     1577880018      normal      training   \n",
       "4                   3     user-011     1577880032      normal      training   \n",
       "\n",
       "  sim_account  sim_req_mem  sim_req_mem_per_cpu  sim_features  sim_gres  \\\n",
       "0    training         7424                    0           NaN       NaN   \n",
       "1     crucial        22272                    0           NaN       NaN   \n",
       "2    training        22272                    0           NaN       NaN   \n",
       "3     crucial        22272                    0           NaN       NaN   \n",
       "4     crucial        22272                    0           NaN       NaN   \n",
       "\n",
       "   sim_shared  sim_cpus_per_task  sim_dependency  sim_cancelled_ts  freq  pe  \\\n",
       "0           1                  1             NaN                 0    50   1   \n",
       "1           1                  1             NaN                 0    50   3   \n",
       "2           1                  1             NaN                 0    50   3   \n",
       "3           1                  1             NaN                 0    50   3   \n",
       "4           1                  1             NaN                 0    50   3   \n",
       "\n",
       "   job_prio  nodes_req     batch_name  batch_value  JOBINFO_INTERARRIVAL_TIME  \\\n",
       "0         1          1  03_Inter_13.0           13                   1.885638   \n",
       "1        10          1  03_Inter_13.0           13                   7.656786   \n",
       "2         1          1  03_Inter_13.0           13                   2.787696   \n",
       "3        10          1  03_Inter_13.0           13                   5.751022   \n",
       "4        10          1  03_Inter_13.0           13                  13.848242   \n",
       "\n",
       "   JOBINFO_INTERARRIVAL_MEAN JOBINFO_INTERARRIVAL_TYPE  CLUSTINFO_NODE_COUNT  \\\n",
       "0                         13                     exp+1                     1   \n",
       "1                         13                     exp+1                     1   \n",
       "2                         13                     exp+1                     1   \n",
       "3                         13                     exp+1                     1   \n",
       "4                         13                     exp+1                     1   \n",
       "\n",
       "   CLUSTINFO_NODE_CPUS  CLUSTINFO_NODE_MEM_MB  CLUSTINFO_NODE_DEF_MEM_PER_CPU  \n",
       "0                    8                  59392                            7424  \n",
       "1                    8                  59392                            7424  \n",
       "2                    8                  59392                            7424  \n",
       "3                    8                  59392                            7424  \n",
       "4                    8                  59392                            7424  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Batch Name\n",
    "import re\n",
    "\n",
    "p = re.compile('(.+)_([^_]+)$') # extract batch name from run_name (bit after last underscore)\n",
    "batch_name = p.match(run_name).group(2)\n",
    "\n",
    "# trace file\n",
    "trace_filename = f\"{run_type}_{batch_name}.csv\"\n",
    "trace_folder = \"/mnt/wsl/instances/Ubuntu-18.04//home/slurm/slurm_sim_ws/slurm_sim_tools/slurm_batches/simple_traces/srun\"\n",
    "trace_path = os.path.join(trace_folder, trace_filename)\n",
    "\n",
    "print(trace_path)\n",
    "\n",
    "df_trace_raw = pd.read_csv(trace_path)\n",
    "\n",
    "pd.options.display.max_columns=100\n",
    "\n",
    "df_trace_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e18b66-a79f-412e-b35d-b5701dfb063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Before Fix:**\n",
      "['user-001' 'user-011']\n",
      "[ 1 10]\n",
      "['training' 'crucial']\n",
      "\n",
      "**After Fix:**\n",
      "['user-001']\n",
      "[1]\n",
      "['training']\n"
     ]
    }
   ],
   "source": [
    "if UPDATE_DATA_TO_FIFO:\n",
    "    print(\"**Before Fix:**\")\n",
    "    print(df_trace_raw.sim_username.unique())\n",
    "    print(df_trace_raw.job_prio.unique())\n",
    "    print(df_trace_raw.sim_account.unique())\n",
    "    \n",
    "    df_trace_raw.loc[df_trace_raw.sim_username == \"user-011\", 'sim_username'] = 'user-001'\n",
    "    df_trace_raw.loc[df_trace_raw.sim_account == \"crucial\", 'sim_account'] = 'training'\n",
    "    df_trace_raw.loc[df_trace_raw.job_prio == 10, 'job_prio'] = 1\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"**After Fix:**\")\n",
    "    print(df_trace_raw.sim_username.unique())\n",
    "    print(df_trace_raw.job_prio.unique())\n",
    "    print(df_trace_raw.sim_account.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abc5842e-7f8d-4981-8a59-6207019ab813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLUSTINFO_NODE_COUNT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLUSTINFO_NODE_CPUS</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLUSTINFO_NODE_MEM_MB</td>\n",
       "      <td>59392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLUSTINFO_NODE_DEF_MEM_PER_CPU</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Parameter  Value\n",
       "0            CLUSTINFO_NODE_COUNT      1\n",
       "1             CLUSTINFO_NODE_CPUS      8\n",
       "2           CLUSTINFO_NODE_MEM_MB  59392\n",
       "3  CLUSTINFO_NODE_DEF_MEM_PER_CPU   7424"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check only 1 type of cluster info\n",
    "\n",
    "csv_cols = df_trace_raw.columns.values.tolist()\n",
    "clustinfo_cols = [x for x in csv_cols if x.startswith(\"CLUSTINFO\")]\n",
    "\n",
    "clust_params = (df_trace_raw\n",
    " .loc[:, clustinfo_cols]\n",
    " .drop_duplicates()\n",
    ")\n",
    "\n",
    "if clust_params.shape[0] != 1: raise Exception(\"Multiple Cluster parameters! Please Debug.\")\n",
    "\n",
    "# Display in table\n",
    "(clust_params\n",
    " .reset_index()\n",
    " .melt(id_vars=['index'], value_vars=clustinfo_cols, var_name = \"Parameter\", value_name='Value')\n",
    " .drop(columns='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d01569b3-8faf-442b-9f68-d66b1fca3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trace_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e2afba-304c-42ca-b5eb-63f57fe742dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_number</th>\n",
       "      <th>job_name</th>\n",
       "      <th>job_pe</th>\n",
       "      <th>job_prio</th>\n",
       "      <th>job_run_time</th>\n",
       "      <th>job_walltime</th>\n",
       "      <th>job_entertime</th>\n",
       "      <th>batch_name</th>\n",
       "      <th>batch_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>1195</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>60</td>\n",
       "      <td>2020-01-01 12:42:38</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>1196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>60</td>\n",
       "      <td>2020-01-01 12:42:50</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>1197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>60</td>\n",
       "      <td>2020-01-01 12:43:18</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>1198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>2020-01-01 12:43:40</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>1199</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>60</td>\n",
       "      <td>2020-01-01 12:43:53</td>\n",
       "      <td>03_Inter_13.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_number  job_name  job_pe  job_prio  job_run_time  job_walltime  \\\n",
       "195         195      1195     3.0         1            28            60   \n",
       "196         196      1196     1.0         1            20            60   \n",
       "197         197      1197     1.0         1            38            60   \n",
       "198         198      1198     1.0         1            21            60   \n",
       "199         199      1199     3.0         1            31            60   \n",
       "\n",
       "          job_entertime     batch_name  batch_value  \n",
       "195 2020-01-01 12:42:38  03_Inter_13.0           13  \n",
       "196 2020-01-01 12:42:50  03_Inter_13.0           13  \n",
       "197 2020-01-01 12:43:18  03_Inter_13.0           13  \n",
       "198 2020-01-01 12:43:40  03_Inter_13.0           13  \n",
       "199 2020-01-01 12:43:53  03_Inter_13.0           13  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Parameters\n",
    "NODE_COUNT = clust_params.CLUSTINFO_NODE_COUNT.values[0]\n",
    "NODE_CPUS = clust_params.CLUSTINFO_NODE_CPUS.values[0]\n",
    "NODE_MEM_MB = clust_params.CLUSTINFO_NODE_MEM_MB.values[0]\n",
    "DEFAULT_MEM_PER_CPU = clust_params.CLUSTINFO_NODE_DEF_MEM_PER_CPU.values[0]\n",
    "\n",
    "def calc_pe_req(cpus_req, mem_req, nodes_req):\n",
    "    pe_max = NODE_CPUS * NODE_COUNT\n",
    "    \n",
    "    job_pe_req = max([cpus_req / NODE_CPUS, mem_req / NODE_MEM_MB]) * nodes_req * NODE_CPUS\n",
    "    \n",
    "    return job_pe_req\n",
    "\n",
    "def create_trace_format(df):\n",
    "    \n",
    "    df.rename(columns = {\"sim_job_id\" : \"job_name\"},inplace=True)\n",
    "    \n",
    "    if 'job_prio' in df.columns:\n",
    "#         df.rename(columns = {\"sprio_priority\" : \"job_prio\"},inplace=True)\n",
    "        pass\n",
    "    else:\n",
    "        warnings.warn('Missing Job Priority Column, assuming fifo so setting job_prio to decreasing!')\n",
    "        df = df.assign(job_prio = lambda x: -1 * x.job_name)\n",
    "    \n",
    "        \n",
    "    # Runtimes: Both in Seconds\n",
    "    df['job_run_time'] = df.sim_duration\n",
    "    df['job_walltime'] = df.sim_wclimit * 60   # walltime specified in min.\n",
    "        \n",
    "    # Calculate PE requested\n",
    "    df['job_cpus_req'] = df.sim_tasks * df.sim_cpus_per_task\n",
    "    df['default_mem_req'] = df.job_cpus_req * DEFAULT_MEM_PER_CPU\n",
    "    df['job_mem_req'] = df.sim_req_mem.fillna(df.default_mem_req)\n",
    "    df['job_nodes_req'] = df.nodes_req # df.sim_tasks / df.sim_tasks_per_node\n",
    "    \n",
    "    df['job_pe'] = df.apply(lambda x: calc_pe_req(x.job_cpus_req, x.job_mem_req, x.job_nodes_req), axis=1)\n",
    "          \n",
    "    # Date times\n",
    "    df['job_entertime_ts'] = df.sim_submit_ts\n",
    "    df['job_entertime'] = pd.to_datetime(df.sim_submit_ts, unit='s')\n",
    "    \n",
    "    # Select columns\n",
    "    df = (df\n",
    "          .sort_values(['job_entertime', 'job_name'], ascending=[True,True])\n",
    "          .assign(job_number = lambda x: x.index)\n",
    "          .loc[:, ['job_number', 'job_name', 'job_pe', 'job_prio', 'job_run_time', 'job_walltime', \n",
    "                    \"job_entertime\", \n",
    "                    #\"job_entertime_ts\", \n",
    "                    # \"job_cpus_req\", \"job_mem_req\", \"job_nodes_req\"\n",
    "#                     \"job_interarrival_mean\"\n",
    "                   \"batch_name\",\n",
    "                   \"batch_value\"\n",
    "                   ]]\n",
    "         )\n",
    "          \n",
    "    return(df)\n",
    "    \n",
    "# Create job trace dataframe\n",
    "df_job_trace = (df_trace_raw\n",
    "            .pipe(create_trace_format)\n",
    "           )\n",
    "\n",
    "# Convert to dictionary\n",
    "job_trace = df_job_trace.to_dict(orient='records')\n",
    "\n",
    "df_job_trace.head()\n",
    "df_job_trace.tail()\n",
    "\n",
    "## AADD PRIORITY THINGIES!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e55ad0-f6eb-4a3d-9047-cf60711d8a65",
   "metadata": {},
   "source": [
    "# Create Custom Job Trace (roster) -> not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "976fdb09-219d-45a6-b4bd-747c2911bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(type_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7d09fb-ddcb-44d3-9290-0191ced0e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(type_array)\n",
    "# used_type_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb8a5844-00f2-4393-9504-b33c991c558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_seed = 42\n",
    "\n",
    "run_descript = 'pe1_pe5_diff_prio'\n",
    "\n",
    "shuffle_type = \"random\"\n",
    "shuffle_type = \"alternate\"\n",
    "\n",
    "job_type_count = 5\n",
    "\n",
    "job_types = {}\n",
    "\n",
    "job_types[0] = {'job_pe' : 1 , 'job_prio': 10, 'job_run_time': 3}\n",
    "job_types[1]=  {'job_pe' : 1, 'job_prio': 10, 'job_run_time': 4}\n",
    "job_types[2] = {'job_pe' : 1, 'job_prio': 10, 'job_run_time': 5}\n",
    "job_types[3] = {'job_pe' : 4, 'job_prio': 200, 'job_run_time': 5}\n",
    "job_types[4] = {'job_pe' : 5, 'job_prio': 100, 'job_run_time': 5}\n",
    "\n",
    "type_list = [x for x in range(job_type_count)]\n",
    "type_freq = [20,20,20,20,20]\n",
    "# type_freq = [0,100,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5586d394-d34c-4236-b796-47748549e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Job Roster: Helper Functions\n",
    "\n",
    "def rename_job_type_field(job_types, curr_field, new_field):\n",
    "\n",
    "    \"\"\"Renames a job_type field e.g. job_prio to job_priority\"\"\"\n",
    "    \n",
    "    for i in job_types:\n",
    "        curr_type = job_types[i]\n",
    "        renamed_type = {}\n",
    "\n",
    "        for k, v in curr_type.items():\n",
    "            if k == curr_field:\n",
    "                k = new_field\n",
    "\n",
    "            renamed_type[k] = v\n",
    "\n",
    "        job_types[i] = renamed_type\n",
    "\n",
    "    return(job_types)\n",
    "\n",
    "\n",
    "# Write a job roster function ... \n",
    "\n",
    "def make_job_roster(job_types, type_freq, n = None, \n",
    "                    shuffle_type = \"random\", shuffle_seed = 42, \n",
    "                    roster_format = None,\n",
    "                    convert_to_df = False,\n",
    "                    add_job_name = True,\n",
    "                    add_job_enter_time = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a job roster from a number of job types\n",
    "    \n",
    "    type_freq : number of jobs of each type\n",
    "    \n",
    "    shuffle_type : \"random\"\n",
    "                   \"alternate\"  e.g. 1,2,3,1\n",
    "                   \n",
    "    roster_format:  pending -> for Pending Queue format, output as Dataframe\n",
    "                    active -> for Active Queue format, output as Dataframe\n",
    "                    \n",
    "    convert_to_df:  converts output to dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    job_type_count = len(job_types)\n",
    "    \n",
    "    type_list = [x for x in range(job_type_count)]    \n",
    "    # type_freq = [0,100,0,0,0]\n",
    "    \n",
    "    if not n is None:\n",
    "        type_freq = np.round(np.array(type_freq) / np.sum(type_freq) * n)\n",
    "        type_freq = [int(x) for x in type_freq]\n",
    "\n",
    "    if roster_format == \"pending\":\n",
    "        add_job_enter_time = True\n",
    "        convert_to_df = True\n",
    "        \n",
    "        # Fix job type names\n",
    "        job_types = rename_job_type_field(job_types, \"job_prio\", \"job_priority\")\n",
    "        job_types = rename_job_type_field(job_types, \"job_run_time\", \"job_runtime\")\n",
    "        \n",
    "        \n",
    "    all_jobs = []\n",
    "    job_roster = []\n",
    "\n",
    "    if shuffle_type == \"random\":\n",
    "\n",
    "        for j_type, freq in zip(type_list, type_freq):\n",
    "            # print(f\"{type} {freq}\")\n",
    "            for _ in range(freq):\n",
    "                all_jobs.append(j_type)\n",
    "\n",
    "        random.Random(shuffle_seed).shuffle(all_jobs)\n",
    "\n",
    "        for i, jt in enumerate(all_jobs):\n",
    "            job = {}\n",
    "            \n",
    "            # Add Starting Columns\n",
    "            if add_job_name:\n",
    "                job['job_name'] = i\n",
    "            \n",
    "            # Add job_type details\n",
    "            job.update(job_types[jt])\n",
    "            \n",
    "            # Add Columns add end\n",
    "            if add_job_enter_time:\n",
    "                job['job_entertime'] = i+1\n",
    "                \n",
    "                # Example code for changing to datetime! (See 221 notebook for test)\n",
    "                # start_date = datetime.datetime(2020,11,1,12,0,0)\n",
    "                # delta_hours = datetime.timedelta(minutes=i**2)\n",
    "                # job['job_entertime'] = start_date + delta_hours\n",
    "            \n",
    "            \n",
    "            # Add to roster\n",
    "            job_roster.append(job)\n",
    "\n",
    "    elif shuffle_type == \"alternate\": ## ignores type_freq.. assumes equal proportions\n",
    "\n",
    "        type_array = np.array(type_freq)        \n",
    "        used_type_idx = np.where(type_array > 0)[0]\n",
    "        used_type_counts = len(used_type_idx)    \n",
    "        batches = int(type_array.sum() / used_type_counts)\n",
    "        \n",
    "        count = 0\n",
    "        for i in range(batches):\n",
    "            count += 1\n",
    "            for j in used_type_idx:\n",
    "                job = {}                                \n",
    "                \n",
    "            # Add Starting Columns\n",
    "            if add_job_name: \n",
    "                job['job_name'] = i                            \n",
    "            \n",
    "            # Add job_type details\n",
    "            job.update(job_types[jt])\n",
    "            \n",
    "            # Add Columns add end\n",
    "            if add_job_enter_time:\n",
    "                job['job_entertime'] = i+1\n",
    "                \n",
    "                # Example code for changing to datetime! (See 221 notebook for test)\n",
    "                # start_date = datetime.datetime(2020,11,1,12,0,0)\n",
    "                # delta_hours = datetime.timedelta(minutes=i**2)\n",
    "                # job['job_entertime'] = start_date + delta_hours\n",
    "            \n",
    "            # Add to roster\n",
    "            job_roster.append(job)\n",
    "            \n",
    "    if convert_to_df:\n",
    "        job_roster = pd.DataFrame(job_roster)\n",
    "                \n",
    "    return(job_roster)\n",
    "\n",
    "def add_job_entertime(job_roster):\n",
    "    \n",
    "    \"\"\"Adds a simple job enter time column to a job roster\n",
    "    \n",
    "       Currently delay between job is 1 second, starting at 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_roster = job_roster.copy()\n",
    "    \n",
    "    for i, job in enumerate(job_roster):\n",
    "        job['job_entertime'] = i+1\n",
    "        \n",
    "        # Example code for changing to datetime! (See 221 notebook for test)\n",
    "        # start_date = datetime.datetime(2020,11,1,12,0,0)\n",
    "        # delta_hours = datetime.timedelta(minutes=i**2)\n",
    "        # job['job_entertime'] = start_date + delta_hours\n",
    "        \n",
    "    return(new_roster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f9f2831-0bd4-4a5d-a169-bd48febe183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_name</th>\n",
       "      <th>job_pe</th>\n",
       "      <th>job_priority</th>\n",
       "      <th>job_runtime</th>\n",
       "      <th>job_entertime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_name  job_pe  job_priority  job_runtime  job_entertime\n",
       "0         0       1            10            5              1\n",
       "1         1       1            10            5              2\n",
       "2         2       5           100            5              3\n",
       "3         3       1            10            3              4\n",
       "4         4       4           200            5              5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_roster = make_job_roster(job_types, type_freq, n = 100, roster_format = 'pending')\n",
    "\n",
    "job_roster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a9c51-94bd-44db-b7e7-1945222492cb",
   "metadata": {},
   "source": [
    "# Backfill Scheduler: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "346d7796-4f50-4c46-970d-a50a47f0e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activeQ_to_df(activeQ):\n",
    "    act_list = activeQ.as_list()\n",
    "    \n",
    "    act_dic = [{\"job_name\" : j.name(),\n",
    "              \"job_pe\" : j.pe,\n",
    "              \"job_priority\" : j.prio,\n",
    "              \"job_runtime\" : j.run_time,\n",
    "              \"job_start\" : j.enter_time(activeQ)} for j in act_list]\n",
    "\n",
    "    act_df = (pd.DataFrame(act_dic)\n",
    "              .reset_index(drop=True)\n",
    "             )\n",
    "    \n",
    "    if act_df.shape[0] > 0:\n",
    "        act_df = (act_df\n",
    "                  .eval('job_end = job_start + job_runtime')\n",
    "                  .sort_values(['job_end', 'job_priority'],\n",
    "                               ascending = [True, False])\n",
    "                 )\n",
    "    \n",
    "    return(act_df)\n",
    "\n",
    "\n",
    "def pendingQ_to_df(pendingQ, sort = False, sort_columns = None, sort_asc=None):\n",
    "    \n",
    "    if sort_columns is None:\n",
    "        sort_columns = SORT_COLUMNS\n",
    "        \n",
    "    if sort_asc is None:\n",
    "        sort_asc = SORT_ASC\n",
    "    \n",
    "    q_list = pendingQ.as_list()\n",
    "    q_dic = [{\"job_name\" : j.name(),\n",
    "              \"job_pe\" : j.pe,\n",
    "              \"job_priority\" : j.prio,\n",
    "              \"job_runtime\" : j.run_time,\n",
    "              \"job_entertime\" : j.enter_time(pendingQ)} for j in q_list]\n",
    "\n",
    "    # convert q_list to dataframe\n",
    "    q_df = (pd.DataFrame(q_dic)\n",
    "            .reset_index(drop = True)                            \n",
    "           )\n",
    "    \n",
    "    # sort queue\n",
    "    if sort:\n",
    "        q_df = q_df.sort_values(sort_columns, ascending=sort_asc)\n",
    "    \n",
    "    return(q_df)\n",
    "\n",
    "\n",
    "def move_pending_to_active(job_name, pending_df, active_df, time):\n",
    "\n",
    "#     str_num = f\"job.{num}\"\n",
    "    \n",
    "    df_item = pending_df[pending_df.job_name == job_name].copy()\n",
    "    pending_df = pending_df[pending_df.job_name != job_name].copy()\n",
    "    \n",
    "    dic_item = {\"job_name\" : df_item.job_name.values[0],\n",
    "              \"job_pe\" : df_item.job_pe.values[0],\n",
    "              \"job_priority\" : df_item.job_priority.values[0],\n",
    "              \"job_runtime\" :  df_item.job_runtime.values[0],\n",
    "              \"job_start\" : time,\n",
    "              \"job_end\" : time + df_item.job_runtime.values[0]}\n",
    "    \n",
    "    active_df = pd.concat([active_df,\n",
    "                           pd.DataFrame(dic_item, index= [active_df.index.max()+1])], \n",
    "                          axis = 0)\n",
    "    \n",
    "    active_df = active_df.reset_index(drop = True)\n",
    "    \n",
    "    return pending_df, active_df\n",
    "\n",
    "def build_schedule(t, avail_pe, pending_df, active_df, verbose=False):\n",
    "    \n",
    "    # Read Queues to df\n",
    "    # act_df = activeQ_to_df(activeQ)\n",
    "    # q_df = pendingQ_to_df(pendingQ)\n",
    "    \n",
    "    # Initialise Schedule\n",
    "    \n",
    "    sched = []\n",
    "    index = 0        \n",
    "\n",
    "    item =  {\"t\" : float(t),\n",
    "             \"delta_pe\" : 0,\n",
    "             \"pe_avail\" : avail_pe,             \n",
    "             \"type\" : \"init\",\n",
    "             \"job_number\": np.nan,\n",
    "             \"job_prio\" : np.nan,\n",
    "             \"job_pe\" : np.nan}\n",
    "    \n",
    "    sched.append(item)\n",
    "    \n",
    "    curr_avail_pe = avail_pe\n",
    "    t_now = t\n",
    "    count = 0\n",
    "    \n",
    "    while (len(pending_df) + len(active_df) > 0) and (count <= 100):\n",
    "        hasJobComplete = False\n",
    "        hasPendingStart = False\n",
    "        \n",
    "        if verbose: print(f\"count: {count}, time: {t_now}\")\n",
    "        \n",
    "        # Sort Queues\n",
    "        \n",
    "        if len(active_df) > 0:\n",
    "            active_df = active_df.sort_values(['job_end', 'job_priority'],\n",
    "                               ascending = [True, False])\n",
    "        \n",
    "        if len(pending_df) > 0:\n",
    "            pending_df = pending_df.sort_values(SORT_COLUMNS, ascending=SORT_ASC)\n",
    "        \n",
    "        # Active Queue: Check Job Completion\n",
    "        \n",
    "        if len(active_df) > 0:\n",
    "            next_active_end = active_df.job_end.values[0]\n",
    "            \n",
    "            if next_active_end <= t_now: \n",
    "                hasJobComplete = True\n",
    "                \n",
    "            if verbose: print(f\"hasJobComplete: {hasJobComplete}. next_active_end: {next_active_end}\")\n",
    "                \n",
    "        if hasJobComplete:\n",
    "            complete_job_name = active_df.job_name.values[0]\n",
    "            complete_job = active_df[active_df.job_name == complete_job_name]\n",
    "            active_df = active_df[active_df.job_name != complete_job_name]\n",
    "            \n",
    "            curr_avail_pe = curr_avail_pe + complete_job.job_pe.values[0]\n",
    "            \n",
    "            item =  {\"t\" : float(complete_job.job_end.values[0]),\n",
    "                     \"delta_pe\" : complete_job.job_pe.values[0],\n",
    "                     \"pe_avail\" : curr_avail_pe ,             \n",
    "                     \"type\" : \"end\",\n",
    "                     \"job_number\": complete_job.job_name.values[0],\n",
    "                     \"job_prio\" : complete_job.job_priority.values[0],\n",
    "                     \"job_pe\" : complete_job.job_pe.values[0],\n",
    "                     \"job_runtime\" : complete_job.job_runtime.values[0]}\n",
    "    \n",
    "            sched.append(item)\n",
    "            \n",
    "        # Pending Queue: Check for next job to execute\n",
    "        \n",
    "#         pending_df_filtered = pending_df[pending_df.job_entertime]\n",
    "    \n",
    "        if len(pending_df) > 0:\n",
    "            next_pending_pe = pending_df.job_pe.values[0]\n",
    "            \n",
    "            if next_pending_pe <= curr_avail_pe: \n",
    "                hasPendingStart = True\n",
    "\n",
    "            if verbose: print(f\"hasPendingStart: {hasPendingStart}. pe_req: {next_pending_pe}, avail_pe: {curr_avail_pe}\")\n",
    "\n",
    "            \n",
    "        if hasPendingStart:\n",
    "            pending_job_name = pending_df.job_name.values[0]\n",
    "            pending_job = pending_df[pending_df.job_name == pending_job_name]\n",
    "            \n",
    "            curr_avail_pe = curr_avail_pe - pending_job.job_pe.values[0]\n",
    "            \n",
    "            if verbose: \n",
    "                print(f\"Before Starting Job {pending_job.job_name.values[0]}\")\n",
    "                print(pending_df)\n",
    "                print(active_df)\n",
    "            \n",
    "            pending_df, active_df = move_pending_to_active(pending_job_name, pending_df, active_df, t_now)\n",
    "            \n",
    "            if verbose: \n",
    "                print(f\"After Starting Job {pending_job.job_name.values[0]}\")\n",
    "                print(pending_df)\n",
    "                print(active_df)\n",
    "            \n",
    "            item =  {\"t\" : t_now,\n",
    "                     \"delta_pe\" : pending_job.job_pe.values[0],\n",
    "                     \"pe_avail\" : curr_avail_pe ,             \n",
    "                     \"type\" : \"start\",\n",
    "                     \"job_number\": pending_job.job_name.values[0],\n",
    "                     \"job_prio\" : pending_job.job_priority.values[0],\n",
    "                     \"job_pe\" : pending_job.job_pe.values[0],\n",
    "                     \"job_runtime\" : pending_job.job_runtime.values[0]}\n",
    "    \n",
    "            sched.append(item)\n",
    "        \n",
    "        \n",
    "        # Increment Time\n",
    "        count += 1\n",
    "        t_now = t_now + 1  # 1 second\n",
    "        \n",
    "    # Calc PE Available\n",
    "\n",
    "    df_sched = pd.DataFrame(sched)\n",
    "    # df_sched['pe_avail'] = df_sched.delta_pe.cumsum() + avail_pe\n",
    "    \n",
    "    # print\n",
    "    return df_sched\n",
    "\n",
    "\n",
    "def max_backfill_runtime(df_sched, job_pe, job_priority):\n",
    "\n",
    "    \"\"\"Calculates the maximum amount of time a specific job that is being backfilled can run\n",
    "       before being blocked by a higher priority job\n",
    "    \"\"\"\n",
    "    \n",
    "    init_pe = df_sched[df_sched.type == \"init\"].pe_avail[0]\n",
    "    \n",
    "    if job_pe > init_pe:\n",
    "        return(0)\n",
    "    \n",
    "    df_sched2 = (df_sched\n",
    "                 [(df_sched.job_prio >= job_priority) | \n",
    "                  (df_sched.job_prio.isna())]\n",
    "                 .copy()\n",
    "                )\n",
    "    \n",
    "    sched_times = df_sched2.t.values\n",
    "    sched_pe_avail = df_sched2.pe_avail.values\n",
    "    sched_cum_min = df_sched2.pe_avail.cummin().values\n",
    "    idx_valid = np.argwhere(sched_cum_min>=init_pe)\n",
    "    max_run_time = sched_times[idx_valid[-1]] - sched_times[0]\n",
    "    \n",
    "    return(max_run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "395e8837-fd75-4b2e-b0f2-a37240533bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "638374fb-51c3-4073-976c-68b9cf03f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "624ac8d2-0ded-4049-a613-318ab342d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING CODE\n",
    "\n",
    "# if not pendingQ is None:\n",
    "\n",
    "#     init_pe = 1\n",
    "#     prio_level = 50 # For the job... \n",
    "    \n",
    "#     df_sched = build_schedule(3, init_pe, pendingQ_to_df(pendingQ), activeQ_to_df(activeQ))\n",
    "\n",
    "#     display(df_sched)\n",
    "    \n",
    "#     # Calculate the longest time period\n",
    "    \n",
    "#     df_sched2 = (df_sched\n",
    "#                  [(df_sched.job_prio >= prio_level) | \n",
    "#                   (df_sched.job_prio.isna())]\n",
    "#                  .copy()\n",
    "#                 )\n",
    "    \n",
    "#     sched_times = df_sched2.t.values\n",
    "#     sched_pe_avail = df_sched2.pe_avail.values\n",
    "#     sched_cum_min = df_sched2.pe_avail.cummin().values\n",
    "#     idx_valid = np.argwhere(sched_cum_min>=init_pe)\n",
    "#     max_run_time = sched_times[idx_valid[-1]] - sched_times[0]\n",
    "    \n",
    "#     print(sched_pe_avail)\n",
    "#     print(sched_cum_min)\n",
    "#     print(idx_valid)\n",
    "#     print(idx_valid[-1])\n",
    "#     print(max_run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73c7a676-52df-44a2-9aeb-b45a2d9c8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61081a2e-a530-4c26-a1db-287d4230a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_roster[1]['job_entertime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12b6f33d-f948-4702-9468-24dceb3df995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?sim.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1406d022-f511-4303-a920-90472601e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim.Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61ea8ab6-84d8-43b7-95c5-cdfd63ac9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_roster[1]['job_entertime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ccaecfb-d29c-478b-b394-85d01c945b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.datetime_to_t(job_roster[1]['job_entertime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7683b-e71d-47e0-b973-d62ce1b78084",
   "metadata": {},
   "source": [
    "## Simulation: Using Job Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "639fec07-e1ad-4f16-bdd5-1f7e4ebecb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.datetime_to_t(job_trace.job_entertime[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a271bac-f660-48a5-a832-479d379653ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_trace.job_entertime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c066a78a-f28c-48e0-897b-e3a58df49b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roster Version\n",
    "\n",
    "import salabim as sim\n",
    "\n",
    "class JobReader(sim.Component):\n",
    "    def process(self, job_roster):\n",
    "        for job_spec in job_roster:\n",
    "            # Option1: Random\n",
    "            # yield self.hold(sim.Exponential(mean=JOB_INTER_MEAN).sample()) # self.hold(JOB_INTER)            \n",
    "            \n",
    "            # Option2: Specific float time from file\n",
    "            # Job(at=job_spec['job_entertime'], job_spec=job_spec)\n",
    "            \n",
    "            # Option3: Specific datetime!\n",
    "            yield self.hold(till=env.datetime_to_t(job_spec['job_entertime']))\n",
    "            Job(job_spec=job_spec)\n",
    "\n",
    "class Slurm(sim.Component):\n",
    "    def process(self):\n",
    "        while True:\n",
    "            run_again = False # Should loop until most pe possible is used by pending jobs\n",
    "            \n",
    "            if len(pendingQ) > 0:\n",
    "                \n",
    "                avail_pe = res.available_quantity()\n",
    "                slurm_comment = \"\"\n",
    "                \n",
    "                # Read Active Queue\n",
    "                a_df = activeQ_to_df(activeQ)\n",
    "                \n",
    "                # Read Pending Queue\n",
    "                q_df = pendingQ_to_df(pendingQ)\n",
    "\n",
    "                # Get index of job                \n",
    "                pop_index = q_df.sort_values(SORT_COLUMNS, ascending=SORT_ASC).index[0].tolist()\n",
    "                pe_req = pendingQ[pop_index].pe\n",
    "                \n",
    "                if VERBOSE: \n",
    "                    print(q_df.sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "                    print(\"\")\n",
    "                \n",
    "                if QUEUE_TYPE == \"Backfill\" and pe_req > avail_pe:\n",
    "                    env.print_trace(\"\",\"\",f\"\"\"Enter backfill if statement. \n",
    "                                          job number: {pendingQ[pop_index].name()}\n",
    "                                          job prio: {pendingQ[pop_index].prio}\n",
    "                                          job pe: {pe_req}\n",
    "                                          avail pe: {avail_pe}\"\"\")\n",
    "                                    \n",
    "                    q_df = q_df.assign(can_run = q_df.job_pe <= avail_pe)\n",
    "                        \n",
    "                    if len(q_df) > 0:\n",
    "                        \n",
    "                        canrun_jobs_df = (q_df\n",
    "                                     .query('can_run == True')\n",
    "                                     .sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "                        \n",
    "                        if len(canrun_jobs_df) > 0:\n",
    "                                    \n",
    "                            env.print_trace(\"\",\"\",f\"Performing a backfill\")\n",
    "\n",
    "                            pop_index = canrun_jobs_df.index[0].tolist()                            \n",
    "                            slurm_comment = \"Backfill\"\n",
    "                            pe_req = pendingQ[pop_index].pe\n",
    "                                    \n",
    "                            env.print_trace(\"\",\"\",f\"\"\"New job to backfill. \n",
    "                                                  job number: {pendingQ[pop_index].name()}\n",
    "                                                  job prio: {pendingQ[pop_index].prio}\n",
    "                                                  job pe: {pe_req}\n",
    "                                                  avail pe: {avail_pe}\"\"\")\n",
    "                \n",
    "                \n",
    "                if QUEUE_TYPE == \"Backfill_Prio\" and pe_req > avail_pe:\n",
    "                    env.print_trace(\"\",\"\",f\"\"\"Enter backfill if statement. \n",
    "                                          job number: {pendingQ[pop_index].name()}\n",
    "                                          job prio: {pendingQ[pop_index].prio}\n",
    "                                          job pe: {pe_req}\n",
    "                                          avail pe: {avail_pe}\"\"\")\n",
    "                                                        \n",
    "                    sched_df = build_schedule(env.now(), avail_pe, q_df, a_df)\n",
    "                    \n",
    "                    q_df['max_bf_runtime'] = q_df.apply(lambda x:\n",
    "                                                      max_backfill_runtime(sched_df,\n",
    "                                                                           x.job_pe,\n",
    "                                                                           x.job_priority),\n",
    "                                                      axis = 1)\n",
    "                    \n",
    "                    # calc max backfill runtime...\n",
    "                    \n",
    "                    if len(q_df) > 0:\n",
    "                        \n",
    "                        canrun_jobs_df = (q_df\n",
    "                                     .assign(can_backfill = q_df.job_runtime <= q_df.max_bf_runtime)\n",
    "                                     .query('can_backfill == True')                                     \n",
    "                                     .sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "                        \n",
    "                        if len(canrun_jobs_df) > 0:\n",
    "                                    \n",
    "                            env.print_trace(\"\",\"\",f\"Performing a backfill\")\n",
    "                            \n",
    "                            pop_index = canrun_jobs_df.index[0].tolist()\n",
    "                            slurm_comment = \"Backfill\"\n",
    "                            pe_req = pendingQ[pop_index].pe\n",
    "                                    \n",
    "                            env.print_trace(\"\",\"\",f\"\"\"New job to backfill. \n",
    "                                          job number: {pendingQ[pop_index].name()}\n",
    "                                          job prio: {pendingQ[pop_index].prio}\n",
    "                                          job pe: {pe_req}\n",
    "                                          avail pe: {avail_pe}\"\"\")\n",
    "                            \n",
    "                            canrun_jobs_df = (canrun_jobs_df\n",
    "                                     .drop(index=pop_index)\n",
    "                                     .assign(can_backfill_again = canrun_jobs_df.job_pe + pe_req <= avail_pe)\n",
    "                                     .query('can_backfill_again == True')\n",
    "                                     .sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "                            \n",
    "                            if len(canrun_jobs_df) > 0:\n",
    "                                # Has another job to backfill\n",
    "                                env.print_trace(\"\",\"\",\n",
    "                                                f\"\"\"T {env.now()}: Slurm has another possible backfill: \n",
    "                                                runagain = True\"\"\")\n",
    "                                \n",
    "                                run_again = True\n",
    "                            \n",
    "\n",
    "                if pe_req <= avail_pe:\n",
    "                    self.job = pendingQ.pop(pop_index)\n",
    "                    self.job.slurm_comment = slurm_comment\n",
    "                    \n",
    "                    env.print_trace(\"\",\"\",f\"T {env.now()}: Slurm picks Job: {self.job.sequence_number()}\")                    \n",
    "                    self.job.activate()\n",
    "\n",
    "                    # See if enought pe left for another job:\n",
    "                    \n",
    "                    q_df2 = (q_df\n",
    "                            .drop(index=pop_index)\n",
    "                            .assign(can_run_again = q_df.job_pe + pe_req <= avail_pe)\n",
    "                            .query(\"can_run_again == True\")\n",
    "                            .copy()\n",
    "                           )\n",
    "                    \n",
    "                    if len(q_df2) > 0 :\n",
    "                        \n",
    "                        run_again = True\n",
    "                    \n",
    "                        env.print_trace(\"\",\"\",\n",
    "                                        f\"\"\"T {env.now()}: Slurm has another possible job to run: \n",
    "                                        runagain = True\"\"\")\n",
    "\n",
    "                    \n",
    "                    \n",
    "            if not run_again:\n",
    "                yield self.passivate()\n",
    "            \n",
    "\n",
    "class Job(sim.Component):\n",
    "    def setup(self, job_spec):\n",
    "        self.pe = job_spec['job_pe']\n",
    "        self.prio = job_spec['job_prio'] # higher number -> better priority\n",
    "        self.run_time = job_spec['job_run_time']\n",
    "        self.slurm_comment = \"\"\n",
    "        \n",
    "    def process(self):\n",
    "        env.print_trace(\"\",\"\",f\"Received a Job: {self.sequence_number()}, PE: {self.pe}, Priority: {self.prio}\")\n",
    "        job_submit_mon.tally(self.sequence_number())\n",
    "        \n",
    "        job_prio_mon.tally({'job_number': self.sequence_number(), \n",
    "                            'job_prio' : self.prio})\n",
    "        \n",
    "        job_pe_req_mon.tally({'job_number': self.sequence_number(), \n",
    "                              'job_pe_req' : self.pe})\n",
    "        \n",
    "        avail_pe_mon.tally({'job_number': self.sequence_number(), \n",
    "                            'pe_avail' : res.available_quantity()})\n",
    "            \n",
    "        self.enter_sorted(pendingQ, -1*self.prio)\n",
    "        \n",
    "        if slurm.ispassive():\n",
    "            slurm.activate()\n",
    "            \n",
    "        yield self.passivate()\n",
    "        \n",
    "        # Store Slurm comment\n",
    "        job_slurm_comment_mon.tally({'job_number': self.sequence_number(), \n",
    "                                     'slurm_comment' : self.slurm_comment})\n",
    "        \n",
    "        # Request Job Resources\n",
    "        yield self.request(ticket)\n",
    "        yield self.request((res, self.pe))\n",
    "        self.release(ticket)                \n",
    "        \n",
    "        # Run Job\n",
    "        env.print_trace(\"\",\"\",f\"Started Running Job: {self.sequence_number()}, PE: {self.pe}, Priority: {self.prio}\")\n",
    "        self.enter(activeQ)         \n",
    "        job_start_mon.tally(self.sequence_number())\n",
    "        \n",
    "        # Running Time\n",
    "        yield self.hold(self.run_time)        \n",
    "        \n",
    "        # Completed Job\n",
    "        job_end_mon.tally(self.sequence_number())\n",
    "        self.leave(activeQ)\n",
    "        self.release(res)\n",
    "        \n",
    "        if slurm.ispassive():\n",
    "            slurm.activate()\n",
    "            \n",
    "        env.print_trace(\"\",\"\",f\"Finished Running Job: {self.sequence_number()}, PE: {self.pe}, Priority: {self.prio}\")\n",
    "\n",
    "# env = sim.Environment(datetime0=True, trace = False)\n",
    "# start_time = job_roster[0]['job_entertime'] - datetime.timedelta(seconds=1)\n",
    "\n",
    "env = sim.Environment(datetime0=True, trace=False)\n",
    "\n",
    "pendingQ = sim.Queue(\"pendingQ\")\n",
    "activeQ = sim.Queue(\"activeQ\")\n",
    "\n",
    "ticket = sim.Resource(\"ticket\")\n",
    "res = sim.Resource(\"res\", capacity=CLUSTER_PE_AVAIL)\n",
    "\n",
    "slurm = Slurm()\n",
    "\n",
    "job_submit_mon = sim.Monitor()\n",
    "job_start_mon = sim.Monitor()\n",
    "job_end_mon = sim.Monitor()\n",
    "job_pe_req_mon = sim.Monitor()\n",
    "job_prio_mon = sim.Monitor()\n",
    "job_slurm_comment_mon = sim.Monitor()\n",
    "avail_pe_mon = sim.Monitor()\n",
    "\n",
    "# Roster\n",
    "\n",
    "JobReader(job_roster=job_trace)\n",
    "\n",
    "# BACKFILLING TESTING ROSTER\n",
    "# {'job_pe' : 1 , 'job_prio': 10, 'job_run_time': 10}\n",
    "\n",
    "# # Example 1\n",
    "# res = sim.Resource(\"res\", capacity=5)\n",
    "# run_descript = \"failed_bf_same_time\"\n",
    "\n",
    "# Job(at=1.00, job_spec = {'job_pe' : 4, 'job_prio' : 100, 'job_run_time' : 5}) # expected finish... 6\n",
    "# Job(at=2.00, job_spec = {'job_pe' : 5, 'job_prio' : 150, 'job_run_time' : 5}) # higher priority. expected start ... 6\n",
    "# Job(at=3.00, job_spec = {'job_pe' : 1, 'job_prio' : 100, 'job_run_time' : 3}) # this should be backfilled . expected start 3.. finish at 6\n",
    "# Job(at=3.00, job_spec = {'job_pe' : 1, 'job_prio' : 100, 'job_run_time' : 3}) # this should be backfilled . expected start 3.. finish at 6\n",
    "\n",
    "# # Example 2\n",
    "\n",
    "# delta = 20\n",
    "\n",
    "# Job(at=delta + 1.00,  job_spec = {'job_pe' : 4, 'job_prio' : 100, 'job_run_time' : 5})\n",
    "# Job(at=delta + 2.00,  job_spec = {'job_pe' : 5, 'job_prio' : 150, 'job_run_time' : 5}) # higher priority\n",
    "# Job(at=delta + 3.00,  job_spec = {'job_pe' : 1, 'job_prio' : 100, 'job_run_time' : 4}) ## backfill start... 3... backfill finish... 7\n",
    "#                                                                                        ## this should not be backfilled ... \n",
    "#                                                                                        ## since will delay job 2 (pe 5)\n",
    "\n",
    "# Job(at=delta + 3.01,  job_spec = {'job_pe' : 1, 'job_prio' : 100, 'job_run_time' : 4})\n",
    "\n",
    "# # Idee: Laai sommer direk in job_roster... \n",
    "\n",
    "# Queue Parameters\n",
    "# QUEUE_TYPE = \"FIFO\"\n",
    "# QUEUE_TYPE = \"Priority\"\n",
    "# QUEUE_TYPE = \"Backfill\"\n",
    "# QUEUE_TYPE = \"Backfill\"\n",
    "# QUEUE_TYPE = \"Backfill_Prio\"\n",
    "\n",
    "# Queue Sorting\n",
    "    \n",
    "if QUEUE_TYPE == \"FIFO\":\n",
    "    SORT_COLUMNS = ['job_entertime']\n",
    "    SORT_ASC = [True]\n",
    "    \n",
    "if QUEUE_TYPE == \"LIFO\":\n",
    "    SORT_COLUMNS = ['job_entertime']\n",
    "    SORT_ASC = [False]\n",
    "    \n",
    "elif QUEUE_TYPE == \"Priority\":\n",
    "    SORT_COLUMNS = ['job_priority', 'job_entertime']\n",
    "    SORT_ASC = [False, True]\n",
    "\n",
    "elif QUEUE_TYPE == \"HighPE_Prio\":\n",
    "    SORT_COLUMNS = ['job_pe', 'job_priority']\n",
    "    SORT_ASC = [False, False]\n",
    "    \n",
    "elif QUEUE_TYPE == \"Backfill\":\n",
    "    SORT_COLUMNS = ['job_priority', 'job_entertime']\n",
    "    SORT_ASC = [False, True] # Highest Priority, then First entry time.\n",
    "\n",
    "elif QUEUE_TYPE == \"Backfill_Prio\":\n",
    "    SORT_COLUMNS = ['job_priority', 'job_entertime']\n",
    "    SORT_ASC = [False, True] # Highest Priority, then First entry time.\n",
    "    \n",
    "else: \n",
    "    print(\"Please set the QUEUE_TYPE variable!\")\n",
    "    SORT_COLUMNS = []\n",
    "    SORT_ASC = [] \n",
    "\n",
    "# env.run(till=3)\n",
    "env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7810a55-32c1-4693-a207-ebbeb8bde69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activeQ.print_info()\n",
    "# pendingQ.print_info()\n",
    "# activeQ_to_df(activeQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51b89126-5701-4c5b-ba0e-c4acc0ccdbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pendingQ_to_df(pendingQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "485505e9-917b-4aff-98d8-facf2cd275a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (act_df\n",
    "#  .reset_index(drop=True)\n",
    "#  .assign()\n",
    "#  .assign(job_batch = 0)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ee98b6f-49d1-415b-9881-9ad97cef8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9766172-4d6e-4a14-9200-e5bb04d6a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, job in act_df.iterrows():\n",
    "#     print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47ca73c3-c8a3-45d0-838a-d90463e18f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_df.job_entertime.shift(fill_value=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e99a62c-a751-4c9c-a679-c29bd62a9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sched.delta_pe.cumsum()+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afb2a3f5-42ad-4cca-bcc3-754965ba5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(sched).pe_avail.shift(-1, fill_value = 3)\n",
    "\n",
    "# df_sched = pd.DataFrame(sched)\n",
    "# df_sched['pe_avail'] = df_sched.delta_pe.cumsum() + pe_avail\n",
    "\n",
    "# df_sched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8628859e-c35b-4bc4-bd42-50c39f242933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(act_df)\n",
    "# len(q_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cdf14a5-70af-4941-918f-8a6702c1f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_df2 = q_df.copy() \n",
    "\n",
    "# num = 1\n",
    "# str_num = f\"job.{num}\"\n",
    "# q_df2[q_df2.job_name == str_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6771aec5-45f0-4035-97fc-4723b97e1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(act_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84e4b356-113c-4706-ab38-eac35f369bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pendingQ.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b39f6-b23c-4c7c-abed-a7adaaf66c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activeQ.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d942e8-59af-4bb9-b4cb-e634202738d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_df = pendingQ_to_df(pendingQ)\n",
    "\n",
    "# q_df.set_index(['job_priority', 'job_entertime']).sort_index([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32338123-482a-45f4-ae65-03794964d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avail_pe = res.available_quantity()\n",
    "# slurm_comment = \"\"\n",
    "\n",
    "\n",
    "# df_sched = build_schedule(3, 1, pendingQ_to_df(pendingQ), activeQ_to_df(activeQ), verbose = False)\n",
    "\n",
    "# df_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b867c35-4652-4fba-8964-55c8b9515971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_roster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269798e-e02f-4807-8322-c6d404a2c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_roster_pendingq = []\n",
    "\n",
    "# for i, j in enumerate(job_roster):\n",
    "#     new_item = {\"job_name\" : i,\n",
    "#               \"job_pe\" : j['job_pe'],\n",
    "#               \"job_priority\" : j['job_prio'],\n",
    "#               \"job_runtime\" :  j['job_run_time'],\n",
    "#               \"job_entertime\" : i+1}\n",
    "    \n",
    "#     job_roster_pendingq.append(new_item)\n",
    "    \n",
    "# df_pending_test = pd.DataFrame(job_roster_pendingq)\n",
    "\n",
    "# df_pending_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10464d01-1f6e-4e7c-a7c5-ae1f0de29e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_active_test = activeQ_to_df(activeQ)\n",
    "\n",
    "# df_active_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a9d27-6806-4cfb-827a-772e32e1fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pending_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0283ad-a3d2-4fff-b90d-78d4181cafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORT_COLUMNS = ['job_entertime', 'job_priority'] # MAW FIFO op oomblik ...\n",
    "# SORT_ASC = [True, False]\n",
    "\n",
    "# df_sched = build_schedule(0, 5, df_pending_test, df_active_test, verbose = False)\n",
    "\n",
    "# SORT_COLUMNS = ['job_priority', 'job_entertime'] # Priority\n",
    "# SORT_ASC = [False, True]\n",
    "\n",
    "# df_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae0cbf-2103-456a-82b8-741689074b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70781b0-214b-4b86-8091-a18c3404d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     # Get index of job                \n",
    "#     pop_index = q_df.sort_values(SORT_COLUMNS, ascending=SORT_ASC).index[0].tolist()\n",
    "#     pe_req = pendingQ[pop_index].pe\n",
    "\n",
    "#     if VERBOSE: \n",
    "#         print(q_df.sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "#         print(\"\")                \n",
    "\n",
    "#     if QUEUE_TYPE == \"Backfill\" and pe_req > avail_pe:\n",
    "#         env.print_trace(\"\",\"\",f\"\"\"Enter backfill if statement. \n",
    "#                               job number: {pendingQ[pop_index].name()}\n",
    "#                               job prio: {pendingQ[pop_index].prio}\n",
    "#                               job pe: {pe_req}\n",
    "#                               avail pe: {avail_pe}\"\"\")\n",
    "\n",
    "#         q_df = q_df.assign(can_run = q_df.job_pe <= avail_pe)\n",
    "\n",
    "#         if len(q_df) > 0:\n",
    "\n",
    "#             canrun_jobs_df = (q_df\n",
    "#                          .query('can_run == True')\n",
    "#                          .sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "\n",
    "#             if len(canrun_jobs_df) > 0:\n",
    "\n",
    "#                 env.print_trace(\"\",\"\",f\"Performing a backfill\")\n",
    "\n",
    "#                 pop_index = canrun_jobs_df.index[0].tolist()                            \n",
    "#                 slurm_comment = \"Backfill\"\n",
    "#                 pe_req = pendingQ[pop_index].pe\n",
    "\n",
    "#                 env.print_trace(\"\",\"\",f\"\"\"New job to backfill. \n",
    "#                                       job number: {pendingQ[pop_index].name()}\n",
    "#                                       job prio: {pendingQ[pop_index].prio}\n",
    "#                                       job pe: {pe_req}\n",
    "#                                       avail pe: {avail_pe}\"\"\")\n",
    "\n",
    "\n",
    "#     if QUEUE_TYPE == \"Backfill_Prio\" and pe_req > avail_pe:\n",
    "#         env.print_trace(\"\",\"\",f\"\"\"Enter backfill if statement. \n",
    "#                               job number: {pendingQ[pop_index].name()}\n",
    "#                               job prio: {pendingQ[pop_index].prio}\n",
    "#                               job pe: {pe_req}\n",
    "#                               avail pe: {avail_pe}\"\"\")\n",
    "\n",
    "#         q_df = q_df.assign(can_run = q_df.job_pe <= avail_pe)\n",
    "\n",
    "#         if len(q_df) > 0:\n",
    "\n",
    "#             canrun_jobs_df = (q_df\n",
    "#                          .query('can_run == True')\n",
    "#                          .sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "\n",
    "#             if len(canrun_jobs_df) > 0:\n",
    "\n",
    "#                 env.print_trace(\"\",\"\",f\"Performing a backfill\")\n",
    "\n",
    "#                 pop_index = canrun_jobs_df.index[0].tolist()                            \n",
    "#                 slurm_comment = \"Backfill\"\n",
    "#                 pe_req = pendingQ[pop_index].pe\n",
    "\n",
    "#                 env.print_trace(\"\",\"\",f\"\"\"New job to backfill. \n",
    "#                               job number: {pendingQ[pop_index].name()}\n",
    "#                               job prio: {pendingQ[pop_index].prio}\n",
    "#                               job pe: {pe_req}\n",
    "#                               avail pe: {avail_pe}\"\"\")\n",
    "\n",
    "#     if pe_req <= avail_pe:\n",
    "#         self.job = pendingQ.pop(pop_index)\n",
    "#         self.job.slurm_comment = slurm_comment\n",
    "\n",
    "#         env.print_trace(\"\",\"\",f\"T {env.now()}: Slurm picks Job: {self.job.sequence_number()}\")                    \n",
    "#         self.job.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e0892-833d-4748-add6-213158c1c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pendingQ.print_info()\n",
    "\n",
    "# avail_pe = res.available_quantity()\n",
    "# slurm_comment = \"\"\n",
    "\n",
    "# # Read Active Queue\n",
    "\n",
    "# act_list = activeQ.as_list()\n",
    "# act_dic = [{\"job_name\" : j.name(),\n",
    "#           \"job_pe\" : j.pe,\n",
    "#           \"job_priority\" : j.prio,\n",
    "#           \"job_runtime\" : j.run_time,\n",
    "#           \"job_starttime\" : j.enter_time(activeQ)} for j in act_list]\n",
    "\n",
    "# act_df = (pd.DataFrame(act_dic)\n",
    "#           .reset_index(drop=True)\n",
    "#           .eval('job_endtime = job_starttime + job_runtime')\n",
    "#           .sort_values(['job_endtime', 'job_priority'],\n",
    "#                        ascending = [True, False])\n",
    "#          )\n",
    "\n",
    "# # Read Pending Queue\n",
    "\n",
    "# q_list = pendingQ.as_list()\n",
    "# q_dic = [{\"job_name\" : j.name(),\n",
    "#           \"job_pe\" : j.pe,\n",
    "#           \"job_priority\" : j.prio,\n",
    "#           \"job_runtime\" : j.run_time,\n",
    "#           \"job_entertime\" : j.enter_time(pendingQ)} for j in q_list]\n",
    "\n",
    "# # convert q_list to dataframe\n",
    "# q_df = pd.DataFrame(q_dic).reset_index(drop=True)\n",
    "\n",
    "# # Get index of job                \n",
    "# pop_index = q_df.sort_values(SORT_COLUMNS, ascending=SORT_ASC).index[0].tolist()\n",
    "# pe_req = pendingQ[pop_index].pe\n",
    "\n",
    "# if VERBOSE: \n",
    "#     print(q_df.sort_values(SORT_COLUMNS, ascending=SORT_ASC))\n",
    "#     print(\"\") \n",
    "    \n",
    "# env.now()\n",
    "# avail_pe\n",
    "# act_df\n",
    "# q_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2cfa5-00d2-4657-901e-93136d2bd8f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Simple Test\n",
    "\n",
    "# if test_run == 1:\n",
    "#     QUEUE_TYPE = \"Priority\"\n",
    "#     res = sim.Resource(\"res\", capacity=2)\n",
    "#     Job(at=1, pe = 1, prio = 10)\n",
    "#     Job(at=2, pe = 1, prio = 10)\n",
    "#     Job(at=3, pe = 1, prio = 10)\n",
    "\n",
    "# # Priority Test\n",
    "# if test_run == 2: \n",
    "#     QUEUE_TYPE = \"Priority\"\n",
    "#     res = sim.Resource(\"res\", capacity=1)\n",
    "#     Job(at=1, pe = 1, prio = 10)\n",
    "#     Job(at=2, pe = 1, prio = 10)\n",
    "#     Job(at=3, pe = 1, prio = 100)\n",
    "\n",
    "# # Priority + High PE Test! NB!    \n",
    "# if test_run == 3: \n",
    "#     QUEUE_TYPE = \"Priority\"\n",
    "#     res = sim.Resource(\"res\", capacity=3)\n",
    "#     Job(at=1, pe = 1, prio = 10)\n",
    "#     Job(at=2, pe = 3, prio = 100)\n",
    "#     Job(at=3, pe = 1, prio = 10)\n",
    "\n",
    "# if test_run == 4: \n",
    "#     QUEUE_TYPE = \"Priority\"\n",
    "#     res = sim.Resource(\"res\", capacity=3)\n",
    "#     Job(at=1, pe = 1, prio = 10)\n",
    "#     Job(at=2, pe = 3, prio = 10)\n",
    "#     Job(at=3, pe = 1, prio = 100)\n",
    "\n",
    "# if test_run == 5: \n",
    "#     QUEUE_TYPE = \"HighPE_Prio\"\n",
    "#     res =sim.Resource(\"res\", capacity = 3)\n",
    "#     Job(at=1, pe = 1, prio = 10)\n",
    "#     Job(at=1, pe = 2, prio = 10)\n",
    "#     Job(at=1, pe = 3, prio = 10)\n",
    "\n",
    "# if test_run == 6: \n",
    "#     QUEUE_TYPE = \"HighPE_Prio\"\n",
    "#     res =sim.Resource(\"res\", capacity = 3)\n",
    "#     Job(at=1, pe = 1, prio = 10)\n",
    "#     Job(at=1, pe = 2, prio = 10)\n",
    "#     Job(at=1, pe = 3, prio = 20)\n",
    "#     Job(at=1, pe = 3, prio = 10)\n",
    "\n",
    "# if test_run == 7:\n",
    "#     QUEUE_TYPE = \"Priority\"\n",
    "#     res =sim.Resource(\"res\", capacity = 3)\n",
    "#     Job(at=1, pe = 1, prio = 10)\n",
    "#     Job(at=2, pe = 1, prio = 10)\n",
    "#     Job(at=3, pe = 1, prio = 10)\n",
    "#     Job(at=4, pe = 1, prio = 10)\n",
    "#     Job(at=5, pe = 3, prio = 100)\n",
    "    \n",
    "# # No Backfill base case\n",
    "# if test_run == 8:\n",
    "#     QUEUE_TYPE = \"Priority\"\n",
    "#     res =sim.Resource(\"res\", capacity = 5)\n",
    "    \n",
    "#     Job(at=1, pe = 3, prio = 10, run_time = 10)\n",
    "#     Job(at=2, pe = 3, prio = 10, run_time = 5)\n",
    "#     Job(at=3, pe = 1, prio = 10, run_time = 5)\n",
    "#     Job(at=4, pe = 1, prio = 10, run_time = 5)\n",
    "    \n",
    "# # Desired Backfill - but using priorities\n",
    "# # Disadvantage needs to know when submitting job\n",
    "# if test_run == 9:\n",
    "#     QUEUE_TYPE = \"Priority\"\n",
    "#     res =sim.Resource(\"res\", capacity = 5)\n",
    "    \n",
    "#     Job(at=1, pe = 3, prio = 10, run_time = 10)\n",
    "#     Job(at=2, pe = 3, prio = 10, run_time = 5)\n",
    "#     Job(at=3, pe = 1, prio = 10, run_time = 5)\n",
    "#     Job(at=4, pe = 1, prio = 10, run_time = 5)\n",
    "    \n",
    "    \n",
    "# if test_run == 10:\n",
    "#     QUEUE_TYPE = \"Backfill\"\n",
    "#     res =sim.Resource(\"res\", capacity = 5)\n",
    "    \n",
    "#     Job(at=1, pe = 3, prio = 10, run_time = 10)\n",
    "#     Job(at=2, pe = 3, prio = 10, run_time = 5)\n",
    "#     Job(at=3, pe = 1, prio = 10, run_time = 5)\n",
    "#     Job(at=4, pe = 1, prio = 10, run_time = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311bf17-6aa8-4131-a085-208f5f875d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = pd.to_datetime(job_submit_mon.xt()[1][0],unit='s')\n",
    "dt2 = pd.to_datetime(job_submit_mon.xt()[1][1],unit='s')\n",
    "\n",
    "(dt2-dt1).total_seconds()\n",
    "# datetime.timedelta.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75273ce9-59c3-409d-ae21-e49d28344b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_slurm_comment_mon.tx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba0d2a-ad3b-4b2e-a621-84cd993abc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs_df(job_trace,\n",
    "                   job_submit_mon, \n",
    "                   job_start_mon,\n",
    "                   job_end_mon,\n",
    "                   job_pe_req_mon,\n",
    "                   job_prio_mon,\n",
    "                   job_slurm_comment_mon,\n",
    "                   avail_pe_mon):\n",
    "    \n",
    "    # Create Dataframe - Job Start\n",
    "\n",
    "    df_job_trace = (pd.DataFrame(job_trace)\n",
    "                   .loc[:, ['job_number', 'job_name']]\n",
    "                   .set_index('job_number')\n",
    "                   )\n",
    "    \n",
    "    df_jobs_submit = (pd.DataFrame({'job_number'  : job_submit_mon.xt()[0],\n",
    "                                    'job_submit'  : job_submit_mon.xt()[1]})\n",
    "                      .set_index('job_number')\n",
    "                      .apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "                     )\n",
    "    \n",
    "    # RHO TUNE\n",
    "    \n",
    "    df_job_batch_name = (pd.DataFrame(job_trace)\n",
    "                   .loc[:, ['job_number', 'batch_name', 'batch_value']]\n",
    "                   .set_index('job_number')\n",
    "                   .copy()\n",
    "                   )\n",
    "                      \n",
    "\n",
    "    df_job_start = (pd.DataFrame({'job_number'    : job_start_mon.xt()[0],\n",
    "                                  'job_start'     : job_start_mon.xt()[1]})\n",
    "                    .set_index('job_number')\n",
    "                    .apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "                   )\n",
    "\n",
    "    df_job_end = (pd.DataFrame({'job_number'  : job_end_mon.xt()[0],\n",
    "                                'job_end'  : job_end_mon.xt()[1]})\n",
    "                  .set_index('job_number')\n",
    "                  .apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "                 )\n",
    "\n",
    "    df_job_pe_req = (pd.DataFrame.from_records(job_pe_req_mon.tx()[1])\n",
    "                     .set_index('job_number')\n",
    "                    )\n",
    "    \n",
    "    df_job_prio = (pd.DataFrame.from_records(job_prio_mon.tx()[1])\n",
    "                   .set_index('job_number')\n",
    "                  )\n",
    "    \n",
    "    df_job_slurm_comment = (pd.DataFrame.from_records(job_slurm_comment_mon.tx()[1])  \n",
    "                             .set_index('job_number')\n",
    "                            )\n",
    "    \n",
    "    df_avail_pe_mon = (pd.DataFrame.from_records(avail_pe_mon.tx()[1])\n",
    "                       .set_index('job_number')\n",
    "                      )        \n",
    "    \n",
    "    df_jobs = (df_job_trace\n",
    "               .join(df_jobs_submit, how='left')\n",
    "               .join(df_job_batch_name, how='left')  # RHOTUNE\n",
    "               .join(df_job_start, how = 'left')\n",
    "               .join(df_job_end, how = 'left')\n",
    "               .join(df_job_pe_req, how = 'left')\n",
    "               .join(df_avail_pe_mon, how = 'left')\n",
    "               .join(df_job_prio, how = 'left')                \n",
    "               .join(df_job_slurm_comment, how = 'left')\n",
    "               .eval('wait_time = job_start - job_submit')\n",
    "               .eval('run_time = job_end - job_start')\n",
    "               .query('~job_start.isna()')\n",
    "               .reset_index()            \n",
    "              )\n",
    "    \n",
    "    return(df_jobs)\n",
    "\n",
    "\n",
    "def create_wait_df(cluster): \n",
    "    \n",
    "    df_wait = pd.DataFrame({'time': cluster.requesters().length_of_stay.tx()[0],\n",
    "                            'wait_time': cluster.requesters().length_of_stay.tx()[1]})\n",
    "\n",
    "    df_wait.time = df_wait.time.transform(lambda x: pd.to_datetime(x, unit='s'))\n",
    "    df_wait.loc[0, 'time'] = df_wait.loc[1, 'time'] - datetime.timedelta(seconds=1)\n",
    "    \n",
    "    return(df_wait)\n",
    "\n",
    "def create_active_jobs_df(queue):\n",
    "    \n",
    "    df_active_jobs = pd.DataFrame({'time': queue.length.tx()[0],\n",
    "                                   'job_count': queue.length.tx()[1]})\n",
    "    \n",
    "    df_active_jobs.time = df_active_jobs.time.transform(lambda x: pd.to_datetime(x, unit='s'))\n",
    "    df_active_jobs.loc[0,'time'] = df_active_jobs.loc[1,'time'] - datetime.timedelta(seconds=1)\n",
    "    \n",
    "    return(df_active_jobs)\n",
    "\n",
    "def create_pending_jobs_df(queue):\n",
    "    \n",
    "    df_pending_jobs = pd.DataFrame({'time': queue.length.tx()[0],\n",
    "                                   'job_count': queue.length.tx()[1]})\n",
    "    \n",
    "    df_pending_jobs.time = df_pending_jobs.time.transform(lambda x: pd.to_datetime(x, unit='s'))\n",
    "    df_pending_jobs.loc[0, 'time'] = df_pending_jobs.loc[1, 'time'] - datetime.timedelta(seconds=1)\n",
    "    \n",
    "    return(df_pending_jobs)\n",
    "\n",
    "def create_pe_df(cluster):\n",
    "    \n",
    "    df_pe = pd.DataFrame({'time': cluster.available_quantity.tx()[0],\n",
    "                          'pe_avail': cluster.available_quantity.tx()[1]})\n",
    "    \n",
    "    df_pe.time = df_pe.time.transform(lambda x: pd.to_datetime(x, unit='s'))\n",
    "    df_pe.loc[0, 'time'] = df_pe.loc[1, 'time'] - datetime.timedelta(seconds=1)\n",
    "    \n",
    "    return(df_pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c8b63-d321-4025-94c0-071f79b00c62",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad0d2d-6906-4ea7-b830-687b02bb1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot different graphs\n",
    "\n",
    "df_jobs = create_jobs_df(job_trace, \n",
    "                         job_submit_mon, \n",
    "                         job_start_mon,\n",
    "                         job_end_mon,\n",
    "                         job_pe_req_mon,\n",
    "                         job_prio_mon,\n",
    "                         job_slurm_comment_mon,\n",
    "                         avail_pe_mon)\n",
    "\n",
    "df_pe = create_pe_df(res)\n",
    "df_active_jobs = create_active_jobs_df(activeQ)\n",
    "df_pending_jobs = create_pending_jobs_df(pendingQ)\n",
    "\n",
    "# Convert wait times to seconds\n",
    "df_jobs[['wait_time','run_time']] = df_jobs[['wait_time','run_time']].applymap(lambda x: x.total_seconds())\n",
    "\n",
    "\n",
    "df_job_starting_order = (df_jobs\n",
    " .sort_values('job_start')\n",
    " .reset_index(drop=True)\n",
    " .eval('start_number = index+1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c334f-a3a2-40bd-a633-29ba72a736a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs.head()\n",
    "# df_pe.head()\n",
    "# df_active_jobs.tail()\n",
    "# df_pending_jobs.tail()\n",
    "\n",
    "(df_jobs\n",
    " .sort_values('job_start')\n",
    "#  .eval('job_submit = job_submit.round(0)')\n",
    "#  .eval('job_start = job_start.round(0)')\n",
    "#  .eval('job_end = job_end.round(0)')\n",
    " .reset_index(drop=True)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046b76f-589f-4ac1-a378-230973e909f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if reading file works.\n",
    "\n",
    "base_path=\"/mnt/wsl/instances/Ubuntu-18.04/home/slurm/slurm_sim_ws/sim\"\n",
    "sim_folder=\"ilifu_1node/00_baseline/results\"\n",
    "\n",
    "jobcomp_path=os.path.join(base_path, sim_folder, \"jobcomp.log\")\n",
    "\n",
    "sacct_raw = pd.read_csv(jobcomp_path, delimiter=\"|\")\n",
    "\n",
    "sacct = (sacct_raw \n",
    "         .assign(start_time = lambda x: pd.to_datetime(x.Start))\n",
    "         .assign(eligible_time = lambda x: pd.to_datetime(x.Eligible))\n",
    "         .assign(end_time = lambda x: pd.to_datetime(x.End))\n",
    "         .assign(wait_time = lambda x: x.start_time - x.eligible_time)\n",
    "         .assign(wait_sec = lambda x: x.wait_time.dt.total_seconds())\n",
    "        )\n",
    "\n",
    "pd.options.display.max_columns=100\n",
    "\n",
    "sacct.loc[:, ['JobID', 'Submit', 'Eligible', 'Start', 'Elapsed', 'Timelimit', 'wait_time', \n",
    "              'wait_sec'\n",
    "             ]\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a508a-b7f8-4e1a-b42d-cb1babac8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_jobs[['job_number', 'job_pe_req', 'job_prio', 'run_time', 'job_submit']]\n",
    "# #  .query('job_number <= 2')\n",
    "#  .eval('job_submit = job_submit.round(0)')\n",
    "#  .query('job_number > 2')\n",
    "#  .eval('job_number = job_number - 3')\n",
    "#  .eval('job_submit = job_submit - 20')\n",
    "#  .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56deef01-7e4b-4a12-ba32-25c29d26cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa8d12-2b4e-4bf3-854e-0543f42d68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING TIME AXIS PLOT\n",
    "\n",
    "# fig, ax = plt.subplots(1,1, figsize = [7,7])\n",
    "\n",
    "# plt.step(df_pe.time, df_pe.pe_avail, label = \"Available PE\")\n",
    "# plt.legend()\n",
    "\n",
    "# # ax.xaxis.set_major_locator(DayLocator())\n",
    "# # ax.xaxis.set_minor_locator(HourLocator(range(0, 25, 6)))\n",
    "# # ax.fmt_xdata = DateFormatter('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=range(11,19,1)))\n",
    "# ax.xaxis.set_major_locator(mdates.HourLocator(byhour=range(10,19,2)))\n",
    "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:00'))\n",
    "\n",
    "# # ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:00'))\n",
    "# # plt.gcf().autofmt_xdate()\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('PE Available')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e348933-84e9-423d-b99e-c2dab3c8a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing 2: How to share an axis with datetime formatting!\n",
    "\n",
    "# fig, axs = plt.subplots(2,1, figsize = [7,14], sharex=True, sharey=False)\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=range(11,19,1)))\n",
    "#     ax.xaxis.set_major_locator(mdates.HourLocator(byhour=range(10,19,2)))\n",
    "#     ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:00'))\n",
    "\n",
    "# # Plot 1\n",
    "\n",
    "# # plt.subplot(2, 1, 1)\n",
    "\n",
    "# ax = axs[0]\n",
    "# ax.step(df_pe.time, df_pe.pe_avail, label = \"Available PE\")\n",
    "# ax.set_xlabel('Time')\n",
    "# ax.set_ylabel('PE Available')\n",
    "\n",
    "# # Plot 2\n",
    "\n",
    "# ax = axs[1]\n",
    "\n",
    "# ax.step(df_active_jobs.time, df_active_jobs.job_count, label = \"Running Jobs\")\n",
    "# # plt.legend()\n",
    "\n",
    "# ax.set_xlabel('Time')\n",
    "# ax.set_ylabel('Number of Jobs')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3c06a-5b71-46c6-bec8-6ac8f7844342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigate these:\n",
    "\n",
    "# ?mdates.AutoDateLocator\n",
    "# ?mdates.AutoDateFormatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10284d34-fe5b-40e9-b825-2b1718d7b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b04144-68e9-42e8-a068-104fef267675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38319796-ff40-4026-acba-b356482b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize = [21,14], sharex=True, sharey=False)\n",
    "\n",
    "for ax in axs.flat:\n",
    "#     ax.xaxis.set_minor_locator(mdates.AutoDateLocator(interval_multiples=True))\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator(minticks=1, maxticks=3))\n",
    "#     ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=range(11,19,1))) # Finetune! Or make more auto!\n",
    "#     ax.xaxis.set_major_locator(mdates.HourLocator(byhour=range(9,19,3))) # Finetune! Or make more auto!\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d %H:%M'))\n",
    "\n",
    "    ax.xaxis.set_tick_params(which='both', labelbottom=True)\n",
    "    \n",
    "## Plot 1\n",
    "\n",
    "ax = axs[0,0]\n",
    "\n",
    "ax.step(df_pe.time, df_pe.pe_avail, label = \"Available PE\")\n",
    "ax.legend()\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('PE Available')\n",
    "\n",
    "\n",
    "### Plot 2 \n",
    "\n",
    "ax = axs[0,1]\n",
    "\n",
    "df_jobs_backfill = df_jobs.query('slurm_comment == \"Backfill\"').copy()\n",
    "\n",
    "ax.scatter(df_jobs_backfill.job_start, \n",
    "            df_jobs_backfill.wait_time, \n",
    "            s=100, marker = 'o', c='red', alpha = 0.5,\n",
    "            label = \"Backfilled\")\n",
    "\n",
    "plot2 = ax.scatter(df_jobs.job_start, df_jobs.wait_time, c = df_jobs.job_pe_req)\n",
    "\n",
    "# cbar2 = plt.colorbar(plot2)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Time (Job Start Time)')\n",
    "ax.set_ylabel('Waiting Time')\n",
    "# ax.clabel('PE Requested')\n",
    "\n",
    "### Plot 3\n",
    "\n",
    "ax = axs[0,2]\n",
    "plot3 = ax.scatter(df_jobs.job_submit, df_jobs.wait_time, c = df_jobs.job_pe_req, label = \"Waiting Time\")\n",
    "\n",
    "# cbar3 = plt.colorbar(plot3)\n",
    "# ax.legend()\n",
    "\n",
    "ax.set_xlabel('Job Submit Time')\n",
    "ax.set_ylabel('Waiting Time')\n",
    "# ax.clabel('PE Requested')\n",
    "\n",
    "### Plot 4\n",
    "\n",
    "ax = axs[1,0]\n",
    "ax.step(df_active_jobs.time, df_active_jobs.job_count, label = \"Running Jobs\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Number of Jobs')\n",
    "\n",
    "# Plot 5\n",
    "\n",
    "ax = axs[1,1]\n",
    "ax.step(df_pending_jobs.time, df_pending_jobs.job_count, label = \"Pending Jobs\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Number of Jobs')\n",
    "\n",
    "# Plot 6\n",
    "\n",
    "ax = axs[1,2]\n",
    "# plt.scatter(df_job_starting_order.start_number, df_job_starting_order.job_pe_req, c = df_job_starting_order.job_pe_req, label = 'PE Requested')\n",
    "# # plt.legend()\n",
    "# plt.colorbar()\n",
    "# plt.xlabel('Job start order')\n",
    "# plt.ylabel('PE Requested')\n",
    "# # plt.clabel('PE Requested')\n",
    "\n",
    "plot6 = ax.scatter(df_jobs.job_submit, df_jobs.run_time, c = df_jobs.job_pe_req, label = \"Waiting Time\")\n",
    "# cbar6 = plt.colorbar(plot6)\n",
    "# plt.legend()\n",
    "\n",
    "ax.set_xlabel('Job Submit Time')\n",
    "ax.set_ylabel('Run Time')\n",
    "\n",
    "# fig.colorbar(plot3, ax=axs) # ax=[axs[0,2], axs[1,2]])\n",
    "fig.colorbar(plot3, ax=axs) # ax=[axs[0,2], axs[1,2]])\n",
    "\n",
    "# plt.savefig(f'images/sala_qsim_{QUEUE_TYPE.lower()}_rosterplot_{run_descript}.png', \n",
    "#             facecolor='w', transparent=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d279ec-279c-43d2-aea1-c5367e70f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074a6a1-fd38-4a70-9d01-2237ecacc0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start = (df_jobs[['job_name', 'job_pe_req', 'job_start']]\n",
    "            .rename(columns = {'job_pe_req': 'pe_req', \n",
    "                               'job_start':'event_time'})\n",
    "            .assign(event_id = 10, \n",
    "                    event_type = 'start')\n",
    "            .assign(comment = lambda x: x.job_name)\n",
    "            .assign(pe_delta = lambda x: -1 * x.pe_req)\n",
    "            .drop(columns=['pe_req', 'job_name'])\n",
    "           )\n",
    "\n",
    "df_end = (df_jobs[['job_name', 'job_pe_req', 'job_end']]\n",
    "            .rename(columns = {'job_pe_req': 'pe_req', \n",
    "                               'job_end':'event_time'})\n",
    "            .assign(event_id = 9, \n",
    "                    event_type = 'end')\n",
    "            .assign(comment = lambda x: x.job_name)\n",
    "            .assign(pe_delta = lambda x: +1 * x.pe_req)\n",
    "            .drop(columns=['pe_req', 'job_name'])\n",
    "           )\n",
    "\n",
    "min_event_time = pd.to_datetime(df_start.event_time.min().value)\n",
    "\n",
    "df_initial = pd.DataFrame({'event_time' : [min_event_time],\n",
    "                           'event_id' : [0],\n",
    "                           'event_type' : ['init'],\n",
    "                           'comment' : [np.nan],\n",
    "                           'pe_delta' : [NODE_CPUS]\n",
    "                           })\n",
    "\n",
    "# df_event = pd.concat([df_start \n",
    "df_pe_time = (pd.concat([df_initial, \n",
    "                        df_start, \n",
    "                        df_end])\n",
    "             .sort_values(['event_time', 'event_id', 'comment'])\n",
    "             .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "# calculate available pe\n",
    "\n",
    "df_pe_time['pe_avail'] = df_pe_time.pe_delta.cumsum()\n",
    "df_pe_time['pe_avail_norm'] = df_pe_time.pe_avail /  df_pe_time.pe_avail.max()\n",
    "\n",
    "df_pe_time.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35546256-93ca-4c55-9267-42d46775fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pe_time['event_time'] = (\n",
    "    df_pe_time['event_time'] \n",
    "    - min_event_time \n",
    "    + pd.to_datetime(\"2020-01-01 12:00:00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428e646-0ce5-4d52-a92d-0bd12647fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[14,7])\n",
    "\n",
    "# for ax in axs.flat:\n",
    "# #     ax.xaxis.set_minor_locator(mdates.AutoDateLocator(interval_multiples=True))\n",
    "#     ax.xaxis.set_major_locator(mdates.AutoDateLocator(minticks=3, maxticks=4))\n",
    "# #     ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=range(11,19,1))) # Finetune! Or make more auto!\n",
    "# #     ax.xaxis.set_major_locator(mdates.HourLocator(byhour=range(9,19,3))) # Finetune! Or make more auto!\n",
    "#     ax.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d %H:%M'))\n",
    "\n",
    "# ax.xaxis.set_major_locator(mdates.AutoDateLocator(minticks=3, maxticks=5))\n",
    "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%y-%m-%d %H:%M'))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "\n",
    "## Plot 1\n",
    "\n",
    "# ax = axs[0,0]\n",
    "\n",
    "ax.step(df_pe_time.event_time, \n",
    "        df_pe_time.pe_avail_norm,\n",
    "        where='post',  # Baie NB!\n",
    "        label = \"Salabim sim\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel(r'PE Available / Max PE')\n",
    "\n",
    "# SAVE STUFF\n",
    "if save_data:\n",
    "    pltpath = f'{save_image_path}/{run_type}_{run_short}_{run_model}_pe_time_plot.png'\n",
    "    plt.savefig(pltpath, facecolor='w', transparent=False)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "backfill_count = df_jobs.query(\"slurm_comment == 'Backfill'\").shape[0]\n",
    "print(f\"Total Backfills: {backfill_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f45903-7007-4f68-9dcc-5c4d96b20e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add backfill column to jobs\n",
    "df_jobs['backfill'] = np.where(df_jobs.slurm_comment == 'Backfill', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb5c80-08d6-4b97-86d4-64b955f73256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "if save_data:\n",
    "    df_jobs.to_pickle(f\"{save_data_path}/{run_type}_{run_short}_{run_model}_dfjobs.pkl\")\n",
    "    df_pe_time.to_pickle(f\"{save_data_path}/{run_type}_{run_short}_{run_model}_dfpetime.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508c3eb-1b33-4ea2-8ac6-2434b1035f26",
   "metadata": {},
   "source": [
    "# WAIT TIME PER BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c445cb-7822-4a92-8c62-b8bd8ad9c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "n_warmup_exclude = 0    # exclude the first 10 jobs a batch\n",
    "n_cooldown_exclude = 0   # exclude last jobs of a batch. does not include the batch splitting job\n",
    "\n",
    "# FUNCTION\n",
    "def drop_warmup_cooldown_jobs(df):\n",
    "    if n_cooldown_exclude > 0:\n",
    "        return df.iloc[n_warmup_exclude:-n_cooldown_exclude]\n",
    "    else: \n",
    "        return df.iloc[n_warmup_exclude:]\n",
    "        # drops first n_warmup_exclude rows, and the last n_cooldown_exclude rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455273b-0bae-406e-a9c7-a7b6a9de5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job Batch Info\n",
    "df_job_batch = (df_jobs\n",
    "                .loc[:, ['job_name', 'batch_name', 'batch_value']]\n",
    "                .query(\"batch_value >= 0\")\n",
    "                .groupby(['batch_name', 'batch_value'], group_keys=False)\n",
    "                .apply(drop_warmup_cooldown_jobs)\n",
    "                .copy()\n",
    "                .reset_index(drop=True)\n",
    "               )\n",
    "\n",
    "# Job indices\n",
    "df_job_idx = df_job_batch.loc[:, ['job_name']]\n",
    "\n",
    "# summary of job counts\n",
    "(df_jobs\n",
    " .merge(df_job_idx.assign(STATE = \"USED\"), on=\"job_name\", how=\"inner\")\n",
    " .groupby(['batch_name', 'batch_value', \"STATE\"])\n",
    " .agg({\"job_name\": [\"min\", \"max\", \"count\"]\n",
    "        })\n",
    ") # .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8f0ff-668b-4b1d-86f8-303df83bb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c362d6-7374-488d-9bef-f7cb435c08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_wait=(df_jobs\n",
    " .merge(df_job_idx, on=\"job_name\", how=\"inner\")\n",
    " .loc[:, ['batch_name', 'batch_value', 'job_number', 'job_pe_req', 'job_prio', 'wait_time', 'run_time', 'backfill']]\n",
    " .groupby(['batch_name', 'batch_value'])\n",
    " .agg({\n",
    "       # \"wait_time\": [\"min\", \"mean\", \"max\"],\n",
    "      \"wait_time\": [\"mean\", \"median\"],\n",
    "       \"run_time\" : [\"mean\"],\n",
    "       \"job_pe_req\" : [\"mean\"],\n",
    "       'job_number' : [\"count\"],\n",
    "       \"backfill\" : [\"sum\"]\n",
    "        })\n",
    "#  .rename(columns={\"job_pe_req\":\"jobs\"})\n",
    ")\n",
    "\n",
    "# table_wait.to_csv(f\"tables/{run_name}~SimPE.csv\")\n",
    "\n",
    "# ATTEMPT TO ADD RHO\n",
    "table_wait = table_wait.reset_index(drop=False)\n",
    "table_wait[\"est_lam\"] = (1 / table_wait[\"batch_value\"])\n",
    "table_wait[\"est_c\"] = (NODE_CPUS / table_wait[\"job_pe_req\"][\"mean\"])\n",
    "table_wait[\"est_mu\"] = (1 / table_wait[\"run_time\"][\"mean\"]) * table_wait[\"est_c\"]\n",
    "table_wait[\"est_rho\"] =  table_wait[\"est_lam\"] / table_wait[\"est_mu\"]\n",
    "\n",
    "# table_wait.columns = table_wait.columns.map('_'.join)\n",
    "\n",
    "table_wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153226bb-a172-4575-8a17-4eb32fb5b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_wait2=(df_jobs\n",
    " .merge(df_job_idx, on=\"job_name\", how=\"inner\")\n",
    " .loc[:, ['batch_name', 'batch_value', 'job_number', 'job_pe_req', 'job_prio', 'wait_time', 'run_time', 'backfill']]\n",
    " .groupby(['batch_name', 'batch_value', 'job_prio', 'job_pe_req'])\n",
    " .agg({\n",
    "       # \"wait_time\": [\"min\", \"mean\", \"max\"],\n",
    "      \"wait_time\": [\"mean\"],\n",
    "       \"run_time\" : [\"mean\"],\n",
    "       \"job_pe_req\" : [\"mean\"],\n",
    "       \"job_number\" : [\"count\"],\n",
    "       \"backfill\" : [\"sum\"]\n",
    "        })\n",
    "#  .rename(columns={\"job_pe_req\":\"jobs\"})\n",
    ")\n",
    "\n",
    "# table_wait2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b5923-2688-4d57-899b-6c681d49666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_wait_prio=(df_jobs\n",
    " .merge(df_job_idx, on=\"job_name\", how=\"inner\")\n",
    " .loc[:, ['batch_name', 'batch_value', 'job_pe_req', 'job_prio', 'wait_time', 'run_time', 'backfill']] \n",
    "#  .groupby(['job_prio', 'job_interarrival_mean'])\n",
    " .groupby(['batch_name', 'batch_value', 'job_prio'])\n",
    " .agg({\"wait_time\": [\"mean\"],\n",
    "       \"run_time\" : [\"mean\"],\n",
    "       \"job_pe_req\" : [\"count\",\"mean\"],\n",
    "       \"backfill\" : [\"sum\"]\n",
    "        })\n",
    "#  .rename(columns={\"job_pe_req\":\"jobs\"})\n",
    ")\n",
    "\n",
    "# # ATTEMPT TO ADD RHO\n",
    "# table_wait_prio = table_wait_prio.reset_index(drop=False)\n",
    "# table_wait_prio[\"est_lam\"] = (1 / table_wait_prio[\"batch_value\"])\n",
    "# table_wait_prio[\"est_c\"] = (NODE_CPUS / table_wait_prio[\"job_pe_req\"][\"mean\"])\n",
    "# table_wait_prio[\"est_mu\"] = (1 / table_wait_prio[\"run_time\"][\"mean\"]) * table_wait_prio[\"est_c\"]\n",
    "# table_wait_prio[\"est_rho\"] =  table_wait_prio[\"est_lam\"] / table_wait_prio[\"est_mu\"]\n",
    "\n",
    "# table_wait_prio = table_wait_prio.drop(columns = ['est_lam', 'est_mu'], level=0)\n",
    "\n",
    "# table_wait.to_csv(f\"tables/{run_name}~SimPE.csv\")\n",
    "\n",
    "table_wait_prio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb662b-f58f-47d7-8ee0-e94498ef5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50th Percentile\n",
    "def q50(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def q75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "# 90th Percentile\n",
    "def q90(x):\n",
    "    return x.quantile(0.9)\n",
    "\n",
    "table_wait_prio_pe=(df_jobs\n",
    " .merge(df_job_idx, on=\"job_name\", how=\"inner\") \n",
    " .loc[:, ['batch_name', 'batch_value', 'job_pe_req', 'job_prio', 'wait_time', 'run_time', 'backfill']]\n",
    " .groupby(['batch_name', 'batch_value', 'job_prio', 'job_pe_req'])\n",
    "#  .groupby(['job_prio', 'job_pe_req', 'job_interarrival_mean'])\n",
    " .agg({\"wait_time\": [\"mean\", q75, q90, \"max\"],\n",
    "       \"run_time\" : [\"mean\"],\n",
    "       \"job_pe_req\" : [\"count\"],\n",
    "       \"backfill\" : [\"sum\"]\n",
    "        })\n",
    " .rename(columns={\"job_pe_req\":\"job\"})\n",
    ")\n",
    "\n",
    "# table_wait.to_csv(f\"tables/{run_name}~SimPE.csv\")\n",
    "\n",
    "table_wait_prio_pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549e52a-9102-4b4e-9458-aa418d323ed7",
   "metadata": {},
   "source": [
    "# Estimate arrivals and completion rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dacd12-35fe-410a-a5d3-a67e562650b0",
   "metadata": {},
   "source": [
    "$$\\pi_0 = [\\sum_{k=0}^{c} \\frac{\\lambda^k}{\\mu^k k!} + \\frac{\\lambda^c}{\\mu^c c!}\\sum_{k=c+1}^K \\frac{\\lambda^{k-c}}{\\mu^{k-c} c^{k-c}}]^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d9401-0aa3-4820-a278-c99663722518",
   "metadata": {},
   "source": [
    "$$W_q = \\pi_0 \\frac{\\rho (c\\rho)^c}{\\lambda (1-\\rho)^2 c!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3fe2fd-4355-49ff-ad4b-07aa505eaa57",
   "metadata": {},
   "source": [
    "That's for the finite queue case ... let's try the infinite case.\n",
    "\n",
    "$$P_0 = [\\sum_{m=0}^{c-1} \\frac {(c\\rho)^m}{m!} + \\frac{(c\\rho)^c}{c!(1-\\rho)}]^{-1}$$ \n",
    "\n",
    "$$W_q = P_0 \\frac{\\rho (c\\rho)^c}{\\lambda (1-\\rho)^2 c!}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1801a-4345-4825-9bce-0a52b4b19692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "c = 2\n",
    "lam = 1 / 8  # combined job arrival.\n",
    "mu_1server = 1 / 10  \n",
    "mu = c * mu_1server\n",
    "\n",
    "# example\n",
    "# c = 2 \n",
    "# lam = 1 / 10\n",
    "# mu_1server = 1/15\n",
    "# mu = c * mu_1server\n",
    "\n",
    "rho = lam / mu\n",
    "\n",
    "print(f\"rho: {rho:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a225228-d5f6-4722-855d-b665022b775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_0_term1 = 0\n",
    "\n",
    "for m in range(c-1):\n",
    "    p_0_term1 += (c ** rho)**m / (factorial(m))\n",
    "    \n",
    "p_0_term2 = ((c*rho)**c) / (factorial(c) * (1 - rho))\n",
    "    \n",
    "p_0 = 1 / (p_0_term1 + p_0_term2)\n",
    "\n",
    "print(p_0) # This seems wrong ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b22a87-b7db-4472-bee4-f4dd112c76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = p_0 * (rho * (c * rho) ** c) / (lam * (1 - rho)**2 * factorial(c))\n",
    "print(f\"waiting time: {w_q:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16bd4f-b73a-4cda-865b-7ca7291f3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vlam = [lam, 0]\n",
    "# vmu = [mu, mu]\n",
    "# vrho = [l / m for l,m in zip(vlam, vmu)]\n",
    "# rho = sum(vrho)\n",
    "\n",
    "# R = rho / mu\n",
    "\n",
    "# print(\"Prio: [10, 1]\")\n",
    "# print(f\"lam's: {vlam}\")\n",
    "# print(f\"mu's: {vmu}\")\n",
    "# print(f\"rho's: {vrho}\")\n",
    "# print(f\"rho: {rho}\")\n",
    "# print(\"\")\n",
    "# print(f\"R : {R}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740c5e7-8e5c-4778-9cd8-6f5a0beb2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_high = R / (1-vrho[0])\n",
    "# w_low = R / ((1-vrho[0])*(1-vrho[0]-vrho[1]))\n",
    "\n",
    "# W = [w_high, w_low]\n",
    "\n",
    "# print(f'10s runtime. Rho High={(100*vrho[0]):.0f}. Wait={w_high:.1f}')\n",
    "# print(f'10s runtime. Rho Low={(100*vrho[1]):.0f}: Wait={w_low:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690afdba-cff2-4892-9e58-12cd4ff04c51",
   "metadata": {},
   "source": [
    "# UTILISATION PER BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253cb5a-2644-4b3a-8c5e-16044b5e34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Batch Info to PE time\n",
    "df_saturation=df_pe_time\n",
    "df_saturation['job_name'] = df_saturation['comment'].astype(\"Int64\")\n",
    "\n",
    "df_job_batch_join = df_job_batch.loc[:, ['job_name', 'batch_name', 'batch_value']].copy()\n",
    "df_job_batch_join['job_name'] = df_job_batch_join.job_name.astype('Int64')\n",
    "\n",
    "# Time \n",
    "df_saturation['time_till_next'] = df_saturation.event_time.shift(-1) - df_saturation.event_time\n",
    "df_saturation['seconds_till_next'] = df_saturation.time_till_next.dt.total_seconds()\n",
    "df_saturation['pe_avail_time'] = df_saturation.seconds_till_next * df_saturation.pe_avail\n",
    "df_saturation['pe_avail_time_norm'] = df_saturation.seconds_till_next * df_saturation.pe_avail_norm\n",
    "\n",
    "# Filter on Job Indexes\n",
    "df_saturation_filter=(df_saturation               \n",
    "                .merge(df_job_batch_join, how='inner', on='job_name')\n",
    "              )\n",
    "\n",
    "# Calculate \n",
    "table_rho = (df_saturation_filter\n",
    "#              .groupby(['event_type','job_interarrival_mean'])\n",
    "             .groupby(['batch_name', 'batch_value'])\n",
    "             .agg({\"event_type\": [\"count\"] ,\n",
    "                   \"pe_avail_time\" : [\"sum\"],\n",
    "                   \"pe_avail_time_norm\" : [\"sum\"],\n",
    "                   \"event_time\" : [\"min\", \"max\"]\n",
    "                })\n",
    "            )\n",
    "\n",
    "table_rho.reset_index(drop=False)\n",
    "\n",
    "table_rho[\"time_span\"] = table_rho['event_time']['max'] - table_rho['event_time']['min']\n",
    "table_rho[\"time_span\"] = table_rho.time_span.dt.total_seconds()\n",
    "table_rho[\"avg_PE_avail\"] = table_rho['pe_avail_time']['sum'] / table_rho[\"time_span\"]\n",
    "table_rho[\"avg_PE_avail_norm\"] = table_rho['pe_avail_time_norm']['sum'] / table_rho[\"time_span\"]\n",
    "table_rho[\"avg_PE_used_norm\"] = 1-table_rho[\"avg_PE_avail_norm\"]\n",
    "\n",
    "table_rho[\"job_count\"] = table_rho[\"event_type\"][\"count\"].div(2).astype(int)\n",
    "\n",
    "# Select Columns\n",
    "# table_rho = table_rho.loc[:, [\"job_count\", \"event_time\", \"avg_PE_avail\", \"avg_PE_avail_norm\", \"avg_PE_used_norm\"]]\n",
    "table_rho = table_rho.loc[:, [\"job_count\", \"time_span\", \"avg_PE_used_norm\"]]\n",
    "\n",
    "#Flatten Multi-index (NICE)\n",
    "table_rho.columns = [\"_\".join(c) if c[1] != \"\" else c[0] for c in table_rho.columns.to_flat_index()] \n",
    "\n",
    "# Remove lowest job_interarrival -> since will be biased by cooldown\n",
    "# table_rho = table_rho.loc[table_rho.index != table_rho.index.min(), :]\n",
    "\n",
    "table_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c705c-fa71-407a-95cd-b04338fa8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \n",
    "table_rho_prio = (df_saturation_filter\n",
    "#              .groupby(['event_type','job_interarrival_mean'])\n",
    "             .groupby(['batch_name', 'batch_value'])\n",
    "             .agg({\"event_type\": [\"count\"] ,\n",
    "                   \"pe_avail_time\" : [\"sum\"],\n",
    "                   \"pe_avail_time_norm\" : [\"sum\"],\n",
    "                   \"event_time\" : [\"min\", \"max\"]\n",
    "                })\n",
    "            )\n",
    "\n",
    "table_rho_prio.reset_index(drop=False)\n",
    "\n",
    "table_rho_prio[\"time_span\"] = table_rho_prio['event_time']['max'] - table_rho_prio['event_time']['min']\n",
    "table_rho_prio[\"time_span\"] = table_rho_prio.time_span.dt.total_seconds()\n",
    "table_rho_prio[\"avg_PE_avail\"] = table_rho_prio['pe_avail_time']['sum'] / table_rho_prio[\"time_span\"]\n",
    "table_rho_prio[\"avg_PE_avail_norm\"] = table_rho_prio['pe_avail_time_norm']['sum'] / table_rho_prio[\"time_span\"]\n",
    "table_rho_prio[\"avg_PE_used_norm\"] = 1-table_rho_prio[\"avg_PE_avail_norm\"]\n",
    "\n",
    "table_rho_prio[\"job_count\"] = table_rho_prio[\"event_type\"][\"count\"].div(2).astype(int)\n",
    "\n",
    "# Select Columns\n",
    "table_rho_prio = table_rho_prio.loc[:, \n",
    "                                    [\"job_count\", \"event_time\", \"avg_PE_avail\", \n",
    "                                     \"avg_PE_avail_norm\", \"avg_PE_used_norm\"]]\n",
    "table_rho_prio = table_rho_prio.loc[:, [\"job_count\", \"avg_PE_used_norm\"]]\n",
    "\n",
    "#Flatten Multi-index (NICE)\n",
    "table_rho_prio.columns = [\"_\".join(c) if c[1] != \"\" else c[0] for c in table_rho_prio.columns.to_flat_index()] \n",
    "\n",
    "# table_rho_prio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58c90b-2ecd-447f-9c7f-20206289582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOMETHING SEEMS SLIGHTLY WRONG HERE .... \n",
    "## I SHOULD BE GROUPING BY THE ORDER OF JOBS INSTEAD OF THE job_interarrival_mean\n",
    "## OTHERWISE THE PE_TIME CALC WILL BE WRONG!!!\n",
    "\n",
    "## ALSO: The cooldown jobs should be excluded ideally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728d0dc-1bb5-420d-87fd-7d4a6c6d3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ace597-f930-49cb-8cd4-d27e35f0e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_est_lam = (df_pe_time\n",
    "          .query(\"event_type == 'start'\")\n",
    "          .loc[:, ['event_time', 'event_type', 'job_name']]\n",
    "          .copy()\n",
    "         )\n",
    "\n",
    "df_est_lam['delta'] = df_est_lam.event_time - df_est_lam.event_time.shift(1)\n",
    "df_est_lam['delta'] = df_est_lam.delta.dt.total_seconds()\n",
    "job_start_interval = df_est_lam.delta.mean()\n",
    "\n",
    "PE_max = df_jobs.pe_avail.max()\n",
    "PE_avg = df_jobs.job_pe_req.mean()\n",
    "est_server_count =  PE_max / PE_avg\n",
    "\n",
    "est_job_int = df_est_lam.delta.mean()\n",
    "est_lam = 1 / est_job_int\n",
    "\n",
    "est_job_run = df_jobs.run_time.mean()\n",
    "est_mu = (1 / est_job_run) * est_server_count # hierdie is al wat werk volgens my! \n",
    "est_rho = est_lam / est_mu\n",
    "print(f\"est_c: {est_server_count}\")\n",
    "print(f\"est_job_int: {est_job_int}\")\n",
    "print(f\"est_lam: {est_lam}\")\n",
    "print(f\"est_mu: {est_mu}\")\n",
    "print(f\"rho estimate: {est_rho}\")  # this doesn't seem to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1261ad-62dc-42a9-820e-45a69b0b692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estimate Job Completions\n",
    "# df_est_lam = (df_pe_time\n",
    "#           .query(\"event_type == 'start'\")\n",
    "#           .loc[:, ['event_time', 'event_type', 'job_name']]\n",
    "#           .copy()\n",
    "#          )\n",
    "\n",
    "# df_est_lam['delta'] = df_est_lam.event_time - df_est_lam.event_time.shift(1)\n",
    "# df_est_lam['delta'] = df_est_lam.delta.dt.total_seconds()\n",
    "# job_start_interval = df_est_lam.delta[100:9900].mean()\n",
    "\n",
    "# # del df_est_lam\n",
    "\n",
    "# # Estimate Job interval (over all servers)\n",
    "# df_est_mu = (df_pe_time\n",
    "#           .query(\"event_type == 'end'\")\n",
    "#           .loc[:, ['event_time', 'event_type', 'job_name']]\n",
    "#           .copy()\n",
    "#          )\n",
    "\n",
    "# df_est_mu['delta'] = df_est_mu.event_time - df_est_mu.event_time.shift(1)\n",
    "# df_est_mu['delta'] = df_est_mu.delta.dt.total_seconds()\n",
    "\n",
    "# job_end_interval = df_est_mu.delta[100:9900].mean()\n",
    "\n",
    "# # del df_est_mu\n",
    "\n",
    "# print(f\"job start interval: {job_start_interval}\")\n",
    "# print(f\"job end interval: {job_end_interval}\")\n",
    "# print(f\"rho estimate: {job_end_interval/job_start_interval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882abd3c-4293-4333-b1ec-96a78b9df14d",
   "metadata": {},
   "source": [
    "## Extra info for Prio2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d78a98-1dee-4b9e-9d44-091ba5201890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prio2_info = (df_jobs.merge(df_job_idx, on=\"job_name\", how=\"inner\")\n",
    " .loc[:, ['batch_name', 'batch_value', 'job_pe_req', 'job_prio', 'wait_time', 'run_time', 'backfill']]\n",
    " .groupby(['batch_name', 'batch_value', 'job_pe_req', 'job_prio'])\n",
    " .wait_time\n",
    " .describe(percentiles=[0.25,0.5,0.75,0.9]))\n",
    "                 \n",
    "df_prio2_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9fcd4-39ca-434e-a85f-41312cf810f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_jobs\n",
    "# del df_pe_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
